\chapter{Background} \label{ch:background}

\section{Submodularity} \label{sect:bg_submod}

Modeling notions such as coverage, representativeness, or diversity is an important challenge in many machine learning problems.
These notions are well captured by submodular set functions.
Analogously, supermodular functions capture notions of smoothness, regularity, or cooperation. 
As a result, submodularity and supermodularity have found numerous applications in machine learning problems of discrete nature, akin to concavity and convexity for continuous optimization.

\subsection{Basics}
We consider set functions $F : 2^V \to \mathbb{R}$, where $V$ is a finite ground set of size $|V| = n$.
Without loss of generality, if not otherwise stated, we will hereafter assume that $V = [n] \defeq \{1, 2, \ldots,n\}$.
Adding an element $i$ to a set $S$ results in a difference in the value of $F$ that is called marginal gain, and is defined as follows.
\begin{definition}[Marginal gain]
For any $i \in V$, and $S \subseteq V$, the marginal gain of adding $i$ to $S$ is
\begin{align*}
F(i \mid S) \defeq F(S \cup \{i\}) - F(S).
\end{align*}
\end{definition}

Intuitively, submodularity expresses a notion of diminishing returns; that is, adding an element to a larger set provides less benefit than adding it to a smaller one.
\begin{definition}[Submodularity]
$F$ is submodular if, for any $S \subseteq T \subseteq V$, and any $v \in V \setminus T$, it holds that
\begin{align*}
F(v\mid T) \leq F(v\mid S).
\end{align*}
\end{definition}
\noindent The following is an equivalent definition of submodularity that will also be useful later in the thesis.
\begin{definition}[Submodularity] \label{def:submod}
$F$ if submodular if, for any $A, B \subseteq V$, it holds that
\begin{align*}
F(A \cup B) + F(A \cap B) \leq F(A) + F(B).
\end{align*}
\end{definition}

Supermodularity is defined analogously by reversing the sign of the above inequalities.
\begin{definition}[Supermodularity] \label{def:supermod}
A function $F$ is supermodular if and only if $-F$ is submodular.
\end{definition}

If a function $m$ is both submodular and supermodular, then it is called modular.
Modular functions can be seen as the discrete analogue of linear continuous functions, and can be defined using a sum over real-numbered ``utilities''.
\begin{definition}[Modularity]
A function $m$ is called modular if it is both submodular and supermodular; it can be written as
\begin{align*}
m(S) = c + \sum_{i \in S} m_i,
\end{align*}
where $c \in \mathbb{R}$, and $m_i \in \mathbb{R}$, for all $i \in V$.
\end{definition}

A function is called monotone when adding an element can never decrease its value.
\begin{definition}[Monotonicity]
A function $F$ is monotone if, for any $i \in V$, and $S \subseteq V$, it holds that
\begin{align*}
F(i \mid S) \geq 0.
\end{align*}
\end{definition}
Furthermore, a function $F$ is called normalized if $F(\varnothing) = 0$.
In some of our results we will use the fact that we can separate the non-normalized, and non-monotone parts of any submodular function according to the following decomposition.
\begin{definition}[Submodular decomposition] \label{def:decomp}
Any submodular function $F$ can be decomposed as
\begin{align} \label{eq:decomp}
  F(S) = c + m(S) + f(S),
\end{align}
for all $S \subseteq V$, where $c \in \mathbb{R}$ is a constant, $m$ is a normalized modular function, and $f$ is a normalized monotone submodular function.
\end{definition}
An analogous decomposition using a monotone supermodular function $f$ is possible for any supermodular function $F$ as well.

\subsection{Submodular Maximization}
Perhaps the most celebrated result pertaining to submodular functions is the approximation guarantee for maximizing a monotone submodular function under a cardinality constraint.
Although the maximization problem itself is NP-hard, \cite{nemhauser78} showed that the simple greedy \algoref{alg:greedy}, which repeatedly adds the element with the maximum marginal gain, identifies a solution that is within a factor of $1 - 1/e$ of the optimal value.
\begin{theorem}[\hspace{1sp}\citealp{nemhauser78}]
For any normalized monotone submodular function $F$, the solution $S^*$ returned by \algoref{alg:greedy} satisfies
\begin{align*}
F(S^*) \geq \left(1 - \frac{1}{e}\right) \max_{S \subseteq V, |S| \leq k} F(S).
\end{align*}
\end{theorem}

\begin{algorithm}[tb]
  \setstretch{1.3}
  \DontPrintSemicolon
  \caption{\strut Greedy submodular maximization}
  \label{alg:greedy}
  \vspace{0.5em}
  \SetKwInOut{Input}{Input}
  \Input{Set function $F$, cardinality constraint $k$}
  $S^*$ $\gets$ $\varnothing$\;
  \For{$j = 1$ \KwTo $k$}{
  Select $i^*$ $\in$ $\argmax_{i \in V \setminus S^*} F(i \mid S^*)$\;
  $S^*$ $\gets$ $S^* \cup \{i^*\}$\;
  }
  \Return{$S$}\;
\end{algorithm}

Numerous extensions and generalizations of this result have been studied, including approximation guarantees for the non-monotone setting \citep{feige11,buchbinder14}; for different kinds of constraints, such as matroid \citep{lee09,calinescu11} and knapsack \citep{chekuri11}; and for the adaptive setting \citep{golovin11,gotovos15}.


\section{Discrete Probabilistic Models, Inference, and Learning}
As stated in the introduction, in the interest of venturing beyond discrete optimization, we consider discrete probabilistic models, that is, distributions over finite subsets of the ground set $V$ defined as
\begin{align*}
p(S; \btheta) = \frac{1}{Z(\btheta)} \exp\left( F(S; \btheta) \right),
\end{align*}
for all $S \subseteq V$.
The function $F$ is parameterized by a (possibly to be learned) vector $\btheta$, and $Z(\btheta)$ denotes the normalizing constant of the distribution, which is also often referred to as the partition function, and defined as
\begin{align*}
Z(\btheta) \defeq \sum_{S \subseteq V} \exp\left( F(S; \btheta) \right).
\end{align*}
An alternative and equivalent way of defining distributions of the above form is via binary random vectors $X \in \{0, 1\}^n$.
If we define $V(X) \defeq \sdef{v \in V}{X_v = 1}$, it is easy to see that the distribution $p_X(X) \propto \exp(\beta F(V(X)))$ over binary vectors is isomorphic to the above distribution over sets.
With a slight abuse of notation, we will use $F(X)$ to denote $F(V(X))$, and use $p$ to refer to both distributions.

For large parts of this thesis, we will focus on such distributions with $F$ being submodular or supermodular.
\begin{definition}[Probabilistic submodular model]
A probabilistic submodular model \citep{djolonga14,gotovos15} is a distribution of the form
\begin{align*}
p(S; \btheta) \propto \exp\left( F(S; \btheta) \right),
\end{align*}
for all $S \subseteq V$, where $F$ is a submodular or supermodular function.
\end{definition}
The resulting models of this form are also referred to as log-submodular and log-supermodular distributions respectively.
Note that the most likely configurations in these distributions directly correspond to the maximizers of the corresponding sub- or supermodular function $F$.
Some commonly used discrete models fall under these categories; for example, the standard Ising and Potts models are log-supermodular, while determinantal point processes are log-submodular.
We now present some examples models in more detail.

\begin{example}[Product distribution]
Perhaps the simplest family of such models are log-modular distributions, which describe a collection of independent binary random variables.
Equivalently, they are distributions of the form \eqref{eq:pdef} where $F$ is a modular function, that is, a function of the form $F(S) = c + \sum_{v \in S}m_v$, where $c, m_v \in \mathbb{R}$, for all $v \in V$.
The partition function of a log-modular distribution can be derived in closed form as $Z_m = \exp(c) \prod_{v \in V} \left( 1 + \exp(m_v) \right)$.
Consequently, the corresponding log-modular distribution is
\begin{align*}
  p_m(S) = \frac{\exp\big( \sum_{v \in S} m_v \big)}{\prod_{v \in V} \left( 1 + \exp(m_v) \right)}.
\end{align*}
\end{example}

\begin{example}[Ising model]
\citep{ising}
The (ferromagnetic) Ising model is an example of a log-supermodular model.
In its simplest form, it is defined through an undirected graph $(V, E)$, and a set of pairwise potentials $\sigma_{v,w}(S) \defeq 4(\mathds{1}_{\{v \in S\}}-0.5)(\mathds{1}_{\{w \in S\}}-0.5)$.
Its distribution has the form $p(S) \propto \exp(\beta\sum_{\{v,w\} \in E} \sigma_{v,w}(S))$, and is log-supermodular, because $F(S) = \sum_{\{v,w\} \in E} \sigma_{v,w}(S)$ is supermodular. (Each $\sigma_{v,w}$ is supermodular, and supermodular functions are closed under addition.)
\end{example}

\begin{example}[Determinantal point process]
\citep{lyons03,kulesza12}
Determinantal point processes (DPPs) are examples of log-submodular models.
A DPP is defined via a positive semidefinite matrix $K \in \mathbb{R}^{n \times n}$, and has a distribution of the form $p(S) \propto \det(K_S)$, where $K_S$ denotes the square submatrix indexed by $S$.
Since $F(S) = \ln \det(K_S)$ is a submodular function, $p$ is log-submodular.
\end{example}

\begin{example}[\flid{}]
\citep{tschiatschek16}
Another example of log-submodular models are those defined through facility location functions, which have the form $F(S) = \sum_{\ell \in [L]} \max_{v \in S}w_{v,\ell}$, where $w_{v,\ell} \geq 0$, and are submodular.
If $w_{v,\ell} \in \{0, 1\}$, then $F$ represents a set cover function.
\end{example}

\begin{example}[\fldc{}]
\citep{djolonga16mixed}
\end{example}

Note that, both the facility location model and the Ising model use decomposable functions, that is, functions that can be written as a sum of simpler submodular (resp. supermodular) functions $F_{\ell}$:
\begin{align} \label{eq:fdec}
F(S) = \sum_{\ell \in [L]} F_{\ell}(S).
\end{align}


\subsection{Inference}

Recently, \citet{djolonga14} considered a more general treatment of such models, and proposed a variational approach for performing approximate probabilistic inference for them.

\todo{Exponential family}

\todo{Contrastive divergence}

Iyer and Bilmes \citep{iyer15} recently considered a different class of probabilistic models, called submodular point processes, which are also defined through submodular functions, and have the form $p(S) \propto F(S)$.
They showed that inference in SPPs is, in general, also a hard problem, and provided approximations and closed-form solutions for some subclasses.

\todo{Add Chengtao constrained etc. paper}
\todo{Add DPP -- Rayleigh papers}

\section{Sampling and Mixing Times}

\paragraph{Gibbs sampler.}
One of the most commonly used chains is the (single-site) Gibbs sampler, which adds or removes a single element %of the ground set
at a time.
It first selects uniformly at random an element $v \in V$; subsequently, it adds or removes $v$ to the current state $X_t$ according to the probability of the resulting state.
We denote by $P : \Omega \times \Omega \to \mathbb{R}$ the transition matrix of a Markov chain, that is, for all $S, R \in \Omega$, $P(S, R) \defeq \P\left[ X_{t+1} = R \mid X_t = S \right]$.
Then, if we define
\begin{align*}
p_{S \rightarrow R} = \displaystyle\frac{\exp(F(R))}{\exp(F(R)) + \exp(F(S))},
\end{align*}
and denote by $S \sim R$ states that differ by exactly one element (i.e., $\big||R| - |S|\big| = 1$),
the transition matrix $\Pg$ of the Gibbs sampler is
\begin{align*}
  \Pg(S, R) = 
  \threepartdefo{\displaystyle\frac{1}{n}p_{S \rightarrow R}}{R \sim S}{1 - \displaystyle\sum_{T \sim S} \displaystyle\frac{1}{n}p_{S \rightarrow T}}{R = S}{0}.
\end{align*}

\paragraph{Approximating the log-partition function.}
There are two straightforward methods for estimating the log-partition function using sampling.
The first one, importance sampling (IS) \citep{ais}, assumes that we have a normalized distribution $\pi$ from which we draw $M$ samples $\{x\}$

The second, reverse important sampling (RIS) \citep{ris},

\paragraph{Mixing times.}
Approximating quantities of interest using MCMC methods is based on using time averages to estimate expectations over the desired distribution.
In particular, we estimate the expected value of function $f : \ss \to \mathbb{R}$ by $\E_p[f(X)] \approx (1/T)\sum_{r=1}^{T} f(X_{s + r})$.
For example, to estimate the marginal $p(v \in S)$, for some $v \in V$, we would define $f(x) = \mathds{1}_{\{x_v = 1\}}$, for all $x \in \ss$.
The choice of burn-in time $s$ and number of samples $T$ in the above expression presents a tradeoff between computational efficiency and approximation accuracy.
It turns out that the effect of both $s$ and $T$ is largely dependent on a fundamental quantity of the chain called \emph{mixing time} \cite{levin08}.

The mixing time of a chain quantifies the number of iterations $t$ required for the distribution of $X_t$ to be close to the stationary distribution $\pi$.
More formally, it is defined as $\tme \defeq \min \sdef{t}{d(t) \leq \epsilon}$, where $d(t)$ denotes the worst-case (over the starting state $X_0$ of the chain) total variation distance between the distribution of $X_t$ and $\pi$.
Establishing upper bounds on the mixing time of our Gibbs sampler is, therefore, sufficient to guarantee efficient approximate marginal inference (e.g., see Theorem 12.19 of \citet{levin08}).
\chapter{Learning Prob. Submodular Models} \label{ch:genes}

\section{Introduction}
As discussed at the very beginning of the thesis, learning probabilistic models from data in one of our main motivations.
The probabilistic framework we employ suggests a principled way to estimate the model parameters given a data set, namely by maximizing the model likelihood under the data.
Unfortunately, the maximum likelihood problem for the model class we consider is, in general, non-convex; even worse, evaluating the likelihood function or its gradient with respect to the model parameters boils down to computing expectations over the distribution at hand, which we have already seen to be a hard problem.
We show in this chapter how we can use sampling to approximate the likelihood gradients, and, thus, perform an approximate gradient ascent procedure that converges to a local optimum of the model likelihood.
Then, we focus on applying this learning procedure to the application of modeling the interactions between gene alterations in cancer patients.
We evaluate our method on synthetic and real cancer data, visualize the results in several ways, and compare them to previously proposed statistical methods.


\section{Approximate Maximum Likelihood Learning using Sampling} \label{sect:ml}

As before, we consider a model of the form
\begin{align*}
p(S; \btheta) = \frac{1}{Z(\btheta)} \exp\left( F(S; \btheta) \right),
\end{align*}
parametrized by a vector $\btheta$, which we would like to learn.
Given a data set of $N$ sets, $\mathcal{D} \defeq (D_1,\ldots,D_N)$, $D_1,\ldots,D_N \subseteq V$, the log-likelihood of the model is
\begin{align*}
\ell(\btheta) &\defeq \sum_{i=1}^N \log p(D_i; \btheta)\\
           &= \sum_{i=1}^N \left( F(D_i; \btheta) - \log Z(\btheta) \right)\\
           &= \sum_{i=1}^N F(D_i; \btheta) - N \log Z(\btheta).
\end{align*}
The gradient of the log-likelihood with respect to the parameters $\btheta$ is
\begin{align*}
                 \*g(\btheta) &\defeq \nabla_{\btheta} \ell(\btheta)\\
                              &= \sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - N \nabla_{\btheta} \log Z(\btheta)\\
                              &= \sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - N \frac{1}{Z(\btheta)} \nabla_{\btheta} Z(\btheta)\\
                              &= \sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - N \frac{1}{Z(\btheta)} \nabla_{\btheta} \sum_{S \subseteq V} \exp\left( F(S; \btheta) \right)\\
                              &= \sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - N \sum_{S \subseteq V} \frac{\exp\left( F(S; \btheta)\right)}{Z(\btheta)} \nabla_{\btheta} F(S; \btheta)\\
                              &= \sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - N \sum_{S \subseteq V} p(S; \btheta) \nabla_{\btheta} F(S; \btheta)\\
                              &= \sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - N\,\E_{p}\left[ \nabla_{\btheta} F(S; \btheta) \right]\\
                              &= \frac{1}{N}\sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - \E_{p}\left[ \nabla_{\btheta} F(S; \btheta) \right].
\end{align*}
This shows that the maximum likelihood parameters satisfy a generalized version of the well-known moment matching condition for exponential family models; cf. \cite[Ch. 20]{koller09}.
That is, at the maximum, the empirical mean of the function gradient over the data set will match the expected gradient over the model distribution.

While the expectation term in the log-likelihood gradient is, in general, infeasible to compute exactly, we can straightforwardly approximate it using the sampling methods discussed in the previous chapters.
In particular, if we have drawn samples $\mathcal{S} = \{ S_1,\ldots,S_M \}$, $S_1,\ldots,S_M \subseteq V$, from distribution $p$, we can approximate the gradient $\*g(\btheta)$ by
\begin{align*}
\widetilde{\*g}(\btheta) \defeq \frac{1}{N}\sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - \frac{1}{M}\sum_{i=1}^M \nabla_{\btheta} F(S_i; \btheta).
\end{align*}
We, therefore, propose learning the parameters $\btheta$ using an approximate gradient ascent procedure, which involves alternating between sampling from the current model to compute $\widetilde{\*g}(\btheta)$, and performing a gradient step towards the direction of $\widetilde{\*g}(\btheta)$, as shown in \algoref{alg:grad}.

\begin{algorithm}[tb]
  \setstretch{1.2}
  \caption{Approximate maximum likelihood maximization}
  \label{alg:grad}
    \begin{algorithmic}[1]
      \REQUIRE Data $\mathcal{D}$, iterations $n_{\mathrm{iter}}$, samples $M$, step $(\gamma_i)_i$, gradient oracle $\nabla_{\btheta} F(S; \btheta)$
      \STATE Initialize $\theta$
      \FOR{$i = 1$ \TO $n_{\mathrm{iter}}$}
        \LET{$\mathcal{S}$}{sample $M$ sets from $p(\cdot\,; \theta)$}
        \LET{$\widetilde{\*g}(\btheta)$}{$\frac{1}{N}\sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - \frac{1}{M}\sum_{i=1}^M \nabla_{\btheta} F(S_i; \btheta)$}
        \LET{$\btheta$}{$\btheta + \gamma_i\,\widetilde{\*g}(\btheta)$}
      \ENDFOR
      \RETURN $\btheta$
    \end{algorithmic}
\end{algorithm}

\paragraph{Gradients of the \fldc{} model.}
Since we will be focusing on the \fldc{} model for the remainder of this chapter, we derive here the gradients of its potential function with respect to its parameters.
For simplicity, we assume that we use an equal number of $L$ dimensions for both the repulsive and the attractive matrices.
As a reminder, the \fldc{} model is then defined via the following potential \citep{djolonga16mixed},
\begin{align*}
F(S; \bu, \bw, \bv) = \sum_{i \in S} u_i + \sum_{j=1}^{L} \left(\max_{i \in S} w_{ij} - \sum_{i \in S} w_{ij}\right) - \sum_{j=1}^{L} \left(\max_{i \in S} v_{ij} - \sum_{i \in S} v_{ij}\right).
\end{align*}
$F$ is differentiable almost everywhere, due to the presence of the two ``$\max$'' functions.
For the points where it is not differentiable, we define subgradients that give equal contribution to all elements that belong to the corresponding ``$\argmax$''.
In particular, for all $i \in V$, $j \in [L]$, we have
\begin{align*}
\nabla_{u_i} F(S; \bu, \bw, \bv) &= \ind[i \in S]\\
\nabla_{w_{ij}} F(S; \bu, \bw, \bv) &= \frac{\ind[i \in \argmax_{r \in S} w_{rj}]}{|\argmax_{r \in S} w_{rj}|} - \ind[i \in S]\\
\nabla_{v_{ij}} F(S; \bu, \bw, \bv) &= -\frac{\ind[i \in \argmax_{r \in S} v_{rj}]}{|\argmax_{r \in S} v_{rj}|} + \ind[i \in S].
\end{align*}
\citet{tschiatschek16} used an alternative set of subgradients, involving randomization over the choice of the ``$\argmax$'' at each gradient step.
We have noticed that our choice often results in slightly improved learning performance in practice.


\section{Modeling Gene Interactions in Cancer}
One of the goals of cancer genomics research is identifying so-called driver mutations, that is, somatic mutations that are responsible for various forms of cancer, and distinguishing them from randomly occuring passenger mutations.
While sequencing data from large-scale projects, such as The Cancer Genome Atlas \citep{tcga}, has been available in increasing quantities, analyzing the occurence combinations of mutations is a combinatorially daunting task.

Driver mutations often occur in a limited number of key biological pathways, and it has been observed that multiple mutations involved in the same pathway tend to not occur together in the same patient \citep{yeang08}.
As a result, it is of interest to discover groups of gene alterations that are (approximately) mutually exclusive.
Finding such a group is then be indication that the participating mutations are part of the same cancer-related pathway.
Since most existing pathway databases lack in detail and accuracy, there has been particular interest in \emph{de novo} methods, that is, methods that analyze the existing patient data without using any prior biological knowledge, and try to identify new potentially significant combinations of mutations.
For a general review of the topic, we refer to \cite{raphael14}.

Previous \emph{de novo} methods have used different combinatorial or statistical scores to assess the degree of mutual exclusivity of a group of mutuations.
These are then paired with some discovery algorithm that exhaustively enumerates groups \citep{muex,yeang08}, progressively builds up larger groups from smaller ones \citep{mutex,rme,memo,timex}, or performs a randomized search in the group space \citep{dendrix,multidendrix,comet}.
As a result, these methods either scale poorly in the number of mutations at hand, or require a prior assumption on the exact or maximum size of the groups to be discovered.
In the following sections, we compare our results against the CoMEt algorithm \citep{comet}, which is a state-of-the-art method for discovering multiple groups of mutually exclusive mutations.
While CoMEt requires prespecifying the number and size of groups to be searched for, it is able to produce in the end a consensus of arbitrarily sized groups.

\subsection{Probabilistic Modeling of Gene Mutations}
Assume that we are given a ground set $V = \{1,\ldots,n\}$ of possible gene mutations, and a data set of $N$ patients, $\mathcal{D} \defeq (D_1,\ldots,D_N)$, where $D_i \subseteq V$ is the combination of mutations that were present in patient $i$.
The data is commonly represented in the literature using a binary alteration matrix, as shown in \figref{fig:bamats} (top), where each row represents a mutation, and each column a patient.
Our goal is to discover groups of mutations $M_1, M_2, \ldots$, $M_i \subseteq V$, with the property that mutations that belong to the same group rarely occur together in the same patient.
\figref{fig:bamats} (bottom) depicts a permuted version of the previous matrix, where the highlighted group of four mutations appears to be highly mutually exclusive.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/example1.pdf}\\[1em]
\includegraphics[width=\textwidth]{figures/genes/example1_rep.pdf}\\[1em]
\caption{(top) An example binary alteration matrix, where each shaded entry $(i, j)$ indicates that mutation $i$ occured in patient $j$. (bottom) The same matrix with permuted rows and columns to illustrate the high degree of mutual exclusivity of the highlighted group of four mutations.}
\label{fig:bamats}
\end{figure}

We propose using the patient data $\mathcal{D}$ to learn an \fldc{} model over the mutation space $V$.
Based on the definiton of the \fldc{} potential, we expect the columns of the $\bw$ and $\bv$ matrices to encode groups of repulsive and attractive mutations respectively.
For the purposes of this thesis, we propose extracting potential groups by thesholding each matrix at a specified level; these groups can then be further assessed for mutual exclusivity or co-occurence using some of the previously proposed statistical tests.
More generally, one can perform marginal/conditional inference in the learned model to compute arbitrary probabilistic quantities that may be useful in specific biological applications.

Our approach offers several advantages over previous work.
First, it inherently uses higher-order potentials to directly capture mutation interactions of arbitrary size, without any need to specify the number or sizes of the groups in advance.
Second, in addition to mutual exclusivity, it also models mutation co-occurence, which may also be useful in cancer research \citep{yeang08,raphael14}.
Finally, in terms of computational complexity, the only potentially super-linear component in our learning procedure is the number of samples required to get an accurate gradient approximation.
As we have previously seen, proving general bounds on the mixing time is a hard endeavour, but practical experience shows that our algorithm only takes a few minutes to run on data sets of hundreds of mutations.


\section{Experiments}
In this section, we practically apply our learning algorithm to the problem of modeling gene interactions outlined above.
We begin with providing some more details about each step of the procedure we use to discover mutually exclusive groups of mutations.
The steps for discovering co-occuring groups are completely analogous.

\paragraph{Step 1: Learning the \fldc{} model.}
We use the approximate maximum likelihood method described in \sectref{sect:ml}.
By definition of the \fldc{} model, the elements of matrices $\bw$ and $\bv$ must be non-negative.
To achieve this during learning, we project the entries of $\bw$ and $\bv$ to the positive orthant after each gradient step.
Furthermore, we have found it beneficial in practice to induce sparsity on these matrices, in order to reduce the effect of noisy data on the learned models, and obtain more interpretable solutions.
To this end, we employ an $L_1$ regularization to both $\bw$ and $\bv$ by projecting each row and column of this matrices to the corresponding $L_1$-ball after each gradient step.

We initialize the entries of $\bu$ to the maximum likelihood estimates of the respective product distribution, that is,
\begin{align*}
u_i = \log\left( \frac{f_i}{1 - f_i} \right),
\end{align*}
where $f_i$ is the frequency of element $i \in V$ in the data set $\mathcal{D}$.
We randomly initialize the entries of $\bw$ and $\bv$ by drawing from a uniform distribution $\mathcal{U}[0, 0.01]$.
To avoid duplicate latent dimensions in the two matrices, for the first half of the iterations, we check the columns of $\bw$ and $\bv$ after each gradient step, and reinitialize a column when we detect that its $L_1$ distance to another column of the same matrix is smaller than a predefined threshold.

Unless otherwise stated, we use $n_{\mathrm{iter}} = 2\cdot10^4$ gradient iterations, and $M = 100|V|$ samples per iteration.
We use the combined sampler detailed in \chapref{ch:m3} with a mix of $100$ random sub- and supergradients, and a combination weight of $\alpha_c = 0.5$.
Finally, we use a fixed step size ($\gamma = 5\cdot10^{-4}$) for the first half of the iterations, and a geometrically decreasing step size ($\gamma_i = \gamma r^i$ with $r = 10^{-3/n_{\mathrm{iter}}}$) for the second half.

\paragraph{Step 2: Extracting proposed mutation sets.}
We start by thresholding each column of the learned $\bw$ matrix at a fixed level $w_{\mathrm{th}} = 1.5$.
We then proceed to create a graph that contains one clique of nodes for each group extracted in the previous step.
Our proposed mutation sets consists of all maximal cliques in this constructed graph.
Creating the graph, rather than directly proposing the groups extracted from the matrix columns, can be useful for merging smaller groups of genes that have been encoded in separate columns of $\bw$ during learning.

\paragraph{Step 3: Testing mutual exclusivity.}
We make use of two statistical tests for testing the degree of mutual exclusivity of a mutation group.

The first was proposed by \cite{mutex}, and used as part of the Mutex algorithm.
For each gene in a proposed mutation group, we run Fisher's exact test on the contingency table that results from examining the occurences of that gene in the data set versus the union of all other genes in the group.
This results in one $p$-value per gene in the mutation group, and the output of the test is the maximum of these $p$-values.
We will call this the ``one vs. all'' test, denote it by $T_{\mathrm{ova}}$, and its output by $p_{\mathrm{ova}}$.

The second was proposed by \cite{comet}, and used as part of the CoMEt algorithm.
It generalizes Fisher's exact test to higher-dimensional contingency tables.
In particular, it consists of a null hypothesis of independent hypergeometric distributions, one for each mutation in the group, and uses as a test statistic the sum of patients in which exactly one mutation from the group occurs.
We will call this the ``generalized Fisher'' test, denote it by $T_{\mathrm{gf}}$, and its output by $p_{\mathrm{gf}}$.

\paragraph{Step 4: FDR control.}
For the synthetic experiments, we will want to make a final decision of whether a proposed group is significantly mutually exclusive or not, in order to compare to the ground truth.
For that purpose, we employ the one vs. all test discussed above, and correct for multiple testing by using an online FDR control procedure known as LORD++ \citep{lordpp,lord}.
In contrast to classic offline methods, such as the BH step-up procedure \citep{bh}, online methods can be applied to settings where the hypotheses to be tested are not necessarily known in advance, and my arrive in an arbitrary order.
This is useful in our case, because we want to output maximal mutually exclusive groups, which means that the decision of whether to test a group or not will depend on whether a supergroup has already been rejected or not.
The LORD++ procedure takes as input the significance level $\alpha$ at which we are testing.
For the procedure's ``starting alpha-wealth'' parameter we use $W_0 = 0.8\alpha$.

For the real data experiments, in the absence of ground truth, we take a more exploratory approach, and do not employ multiple testing.
Rather, we illustrate and discuss the most significant discovered groups, as indicated by their $p$-values according to both $T_{\mathrm{ova}}$ and $T_{\mathrm{gf}}$.

\subsection{Synthetic Data}

\setlength\figureheight{0.65\textwidth}
\setlength\figurewidth{0.9\textwidth}
\renewcommand{\subflen}{\textwidth}
\renewcommand{\scspacey}{-0.3em}
\renewcommand{\scspacex}{0.2em}
\begin{figure}[htb]
  \captionsetup[subfigure]{oneside,margin={2em,0em}}
  \centering
  \begin{subfigure}[b]{\subflen}
    \centering
    \input{figures/genes/syn_nsamples.tex}
    %\vspace{\scspacey}
    \caption{Number of subgradients fixed to $r = 20$.}
    \label{fig:syn_nsubg}
  \end{subfigure}\\[2em]
  \begin{subfigure}[b]{\subflen}
    \centering
    \input{figures/genes/syn_nsubg.tex}
    %\vspace{\scspacey}
    \caption{Number of samples fixed to $M = 200$.}
    \label{fig:syn_nsamples}
  \end{subfigure}\\[1em]
  \caption{
    Learning curves on the reduced AML data set for (a) varying number of samples, and (b) varying number of subgradients.
  }
  \label{fig:syn1}
\end{figure}


\setlength\figureheight{0.65\textwidth}
\setlength\figurewidth{0.95\textwidth}
\renewcommand{\subflen}{\textwidth}
\renewcommand{\scspacey}{-0.3em}
\renewcommand{\scspacex}{0.2em}
\begin{figure}[htb]
  \captionsetup[subfigure]{oneside,margin={2em,0em}}
  \centering
  \begin{subfigure}[b]{\subflen}
    \centering
    \input{figures/genes/syn_single.tex}
    %\vspace{\scspacey}
    \caption{}
    \label{fig:syn_single}
  \end{subfigure}\\[2em]
  \begin{subfigure}[b]{\subflen}
    \centering
    \input{figures/genes/syn_multi.tex}
    %\vspace{\scspacey}
    \caption{Adjusted Rand index for $t$ groups of $k$ mutations.}
    \label{fig:syn_multi}
  \end{subfigure}\\[1em]
  \caption{
    \todo{}
  }
  \label{fig:syn2}
\end{figure}

\subsection{Real Cancer Data}

\paragraph{Acute myeloid leukemia (AML)}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/mat_aml.pdf}\\[2em]
\caption{Test}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/aml_1.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_2.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_3.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_4.pdf}\\[2em]
\caption{Test}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/aml_5.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_6.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_7.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_8.pdf}\\[2em]
\caption{Test}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/graph_aml.pdf}\\[2em]
\caption{AML repulsive graph}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/aml_comet1.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_comet2.pdf}\\[2em]
\caption{CoMEt extra groups (probably appendix)}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/aml_2_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_1_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_3_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_5_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_4_a.pdf}\\[2em]
\caption{Test}
\end{figure}

\paragraph{Breast cancer (BRCA)}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/mat_brca.pdf}\\[2em]
\caption{Test}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/brca_1.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_6.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_8.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_2.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_5.pdf}\\[2em]
\caption{BRCA repulsive (I)}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/brca_4.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_3.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_11.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_10.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_7.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_9.pdf}\\[2em]
\caption{BRCA repulsive (II)}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/graph_brca.pdf}\\[2em]
\caption{BRCA repulsive graph}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/brca_1_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_11_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_7_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_8_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_14_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_4_a.pdf}\\[2em]
\caption{BRCA attractive}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/brca_9_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_13_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_5_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_10_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_12_a.pdf}\\[2em]
\caption{BRCA attractive (appendix I)}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/brca_3_a.pdf}\\[2em]
%\includegraphics[width=\textwidth]{figures/genes/brca_15_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_2_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_6_a.pdf}\\[2em]
\caption{BRCA attractive (appendix II)}
\end{figure}

\section{Conclusion}
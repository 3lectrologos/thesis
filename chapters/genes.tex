\chapter{Learning Prob. Submodular Models} \label{ch:genes}

\section{Introduction}
As discussed at the very beginning of the thesis, learning probabilistic models from data in one of our main motivations.
The probabilistic framework we employ suggests a principled way to estimate the model parameters given a data set, namely by maximizing the model likelihood under the data.
Unfortunately, the maximum likelihood problem for the model class we consider is, in general, non-convex; even worse, evaluating the likelihood function or its gradient with respect to the model parameters boils down to computing expectations over the distribution at hand, which we have already seen to be a hard problem.
We show in this chapter how we can use sampling to approximate the likelihood gradients, and, thus, perform an approximate gradient ascent procedure that converges to a local optimum of the model likelihood.
Then, we focus on applying this learning procedure to the application of modeling the interactions between gene alterations in cancer patients.
We evaluate our method on synthetic and real cancer data, visualize the results in several ways, and compare them to previously proposed statistical methods.


\section{Approximate Maximum Likelihood Learning using Sampling}

As before, we consider a model of the form
\begin{align*}
p(S; \btheta) = \frac{1}{Z(\btheta)} \exp\left( F(S; \btheta) \right),
\end{align*}
parametrized by a vector $\btheta$, which we would like to learn.
Given a data set of $N$ sets, $\mathcal{D} \defeq (D_1,\ldots,D_N)$, $D_1,\ldots,D_N \subseteq V$, the log-likelihood of the model is
\begin{align*}
\ell(\btheta) &\defeq \sum_{i=1}^N \log p(D_i; \btheta)\\
           &= \sum_{i=1}^N \left( F(D_i; \btheta) - \log Z(\btheta) \right)\\
           &= \sum_{i=1}^N F(D_i; \btheta) - N \log Z(\btheta).
\end{align*}
The gradient of the log-likelihood with respect to the parameters $\btheta$ is
\begin{align*}
                 \*g(\btheta) &\defeq \nabla_{\btheta} \ell(\btheta)\\
                              &= \sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - N \nabla_{\btheta} \log Z(\btheta)\\
                              &= \sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - N \frac{1}{Z(\btheta)} \nabla_{\btheta} Z(\btheta)\\
                              &= \sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - N \frac{1}{Z(\btheta)} \nabla_{\btheta} \sum_{S \subseteq V} \exp\left( F(S; \btheta) \right)\\
                              &= \sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - N \sum_{S \subseteq V} \frac{\exp\left( F(S; \btheta)\right)}{Z(\btheta)} \nabla_{\btheta} F(S; \btheta)\\
                              &= \sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - N \sum_{S \subseteq V} p(S; \btheta) \nabla_{\btheta} F(S; \btheta)\\
                              &= \sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - N\,\E_{p}\left[ \nabla_{\btheta} F(S; \btheta) \right]\\
                              &= \frac{1}{N}\sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - \E_{p}\left[ \nabla_{\btheta} F(S; \btheta) \right].
\end{align*}
This shows that the maximum likelihood parameters satisfy a generalized version of the well-known moment matching condition for exponential family models; cf. \cite[Ch. 20]{koller09}.
That is, at the maximum, the empirical mean of the function gradient over the data set will match the expected gradient over the model distribution.

While the expectation term in the log-likelihood gradient is, in general, infeasible to compute exactly, we can straightforwardly approximate it using the sampling methods discussed in the previous chapters.
In particular, if we have drawn samples $\mathcal{S} = \{ S_1,\ldots,S_M \}$, $S_1,\ldots,S_M \subseteq V$, from distribution $p$, we can approximate the gradient $\*g(\btheta)$ by
\begin{align*}
\widetilde{\*g}(\btheta) \defeq \frac{1}{N}\sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - \frac{1}{M}\sum_{i=1}^M \nabla_{\btheta} F(S_i; \btheta).
\end{align*}
We, therefore, propose learning the parameters $\btheta$ using an approximate gradient ascent procedure, which involves alternating between sampling from the current model to compute $\widetilde{\*g}(\btheta)$, and performing a gradient step towards the direction of $\widetilde{\*g}(\btheta)$, as shown in \algoref{alg:grad}.

\begin{algorithm}[tb]
  \setstretch{1.2}
  \caption{Approximate maximum likelihood maximization}
  \label{alg:grad}
    \begin{algorithmic}[1]
      \REQUIRE Data $\mathcal{D}$, iterations $n_{\mathrm{iter}}$, samples $M$, step $(\gamma_i)_i$, gradient oracle $\nabla_{\btheta} F(S; \btheta)$
      \STATE Initialize $\theta$
      \FOR{$i = 1$ \TO $n_{\mathrm{iter}}$}
        \LET{$\mathcal{S}$}{sample $M$ sets from $p(\cdot\,; \theta)$}
        \LET{$\widetilde{\*g}(\btheta)$}{$\frac{1}{N}\sum_{i=1}^N \nabla_{\btheta} F(D_i; \btheta) - \frac{1}{M}\sum_{i=1}^M \nabla_{\btheta} F(S_i; \btheta)$}
        \LET{$\btheta$}{$\btheta + \gamma_i\,\widetilde{\*g}(\btheta)$}
      \ENDFOR
      \RETURN $\btheta$
    \end{algorithmic}
\end{algorithm}

\paragraph{Gradients of the \fldc{} model.}
Since we will be focusing on the \fldc{} model for the remainder of this chapter, we derive here the gradients of its potential function with respect to its parameters.
For simplicity, we assume that we use an equal number of $L$ dimensions for both the repulsive and the attractive matrices.
As a reminder, the \fldc{} model is then defined via the following potential \citep{djolonga16mixed},
\begin{align*}
F(S; \bu, \bw, \bv) = \sum_{i \in S} u_i + \sum_{j=1}^{L} \left(\max_{i \in S} w_{ij} - \sum_{i \in S} w_{ij}\right) - \sum_{j=1}^{L} \left(\max_{i \in S} v_{ij} - \sum_{i \in S} v_{ij}\right).
\end{align*}
$F$ is differentiable almost everywhere, due to the presence of the two ``$\max$'' functions.
For the points where it is not differentiable, we define subgradients that give equal contribution to all elements that belong to the corresponding ``$\argmax$''.
In particular, for all $i \in V$, $j \in [L]$, we have
\begin{align*}
\nabla_{u_i} F(S; \bu, \bw, \bv) &= \ind[i \in S]\\
\nabla_{w_{ij}} F(S; \bu, \bw, \bv) &= \frac{\ind[i \in \argmax_{r \in S} w_{rj}]}{|\argmax_{r \in S} w_{rj}|} - \ind[i \in S]\\
\nabla_{v_{ij}} F(S; \bu, \bw, \bv) &= -\frac{\ind[i \in \argmax_{r \in S} v_{rj}]}{|\argmax_{r \in S} v_{rj}|} + \ind[i \in S].
\end{align*}
\citet{tschiatschek16} used an alternative set of subgradients, involving randomization over the choice of the ``$\argmax$'' at each gradient step.
We have noticed that our choice often results in slightly improved learning performance in practice.


\paragraph{Implementation details.}
By definition of the \fldc{} model, the elements of matrices $\bw$ and $\bv$ must be non-negative.
To achieve this during learning, we project the entries of $\bw$ and $\bv$ to the positive orthant after each gradient step.

Furthermore, we have found it beneficial in practice to induce sparsity on these matrices, in order to reduce the effect of noisy data on the learned models, and obtain more interpretable solutions.
To this end, we employ an $L_1$ regularization to both $\bw$ and $\bv$ by projecting each row and column of this matrices to the corresponding $L_1$-ball after each gradient step.

We initialize the entries of $\bu$ to the maximum likelihood estimates of the respective product distribution, that is,
\begin{align*}
u_i = \log\left( \frac{f_i}{1 - f_i} \right),
\end{align*}
where $f_i$ is the frequency of element $i \in V$ in the data set $\mathcal{D}$.
We randomly initialize the entries of $\bw$ and $\bv$ by drawing from a uniform distribution $\mathcal{U}[0, 0.01]$.
To avoid duplicate latent dimensions in the two matrices, we check the columns of $\bw$ and $\bv$ after each gradient step, and reinitialize a column when we detect that its $L_1$ distance to another column of the same matrix is smaller than a predefined threshold.
Finally, we use a fixed step size ($\gamma = 5\times 10^{-4}$) for the first half of the iterations, and a geometrically decreasing step size ($\gamma_i = \gamma r^i$ with $r = 0.999$) for the second half.


\section{Modeling Gene Interactions in Cancer}
One of the goals of cancer genomics research is identifying so-called driver mutations, that is, somatic mutations that are responsible for various forms of cancer, and distinguishing them from randomly occuring passenger mutations.
While sequencing data from large-scale projects, such as The Cancer Genome Atlas \citep{tcga}, has been available in increasing quantities, analyzing the occurence combinations of mutations is a combinatorially daunting task.

Driver mutations often occur in a limited number of key biological pathways, and it has been observed that multiple mutations involved in the same pathway tend to not occur together in the same patient \citep{yeang08}.
As a result, it is of interest to discover groups of gene alterations that are (approximately) mutually exclusive.
Finding such a group is then be indication that the participating mutations are part of the same cancer-related pathway.
Since most existing pathway databases lack in detail and accuracy, there has been particular interest in \emph{de novo} methods, that is, methods that analyze the existing patient data without using any prior biological knowledge, and try to identify new potentially significant combinations of mutations.
For a general review of the topic, we refer to \cite{raphael14}.

Previous \emph{de novo} methods have used different combinatorial or statistical scores to assess the degree of mutual exclusivity of a group of mutuations.
These are then paired with some discovery algorithm that exhaustively enumerates groups \citep{muex,yeang08}, progressively builds up larger groups from smaller ones \citep{mutex,rme,memo,timex}, or performs a randomized search in the group space \citep{dendrix,multidendrix,comet}.
As a result, these methods either scale poorly in the number of mutations at hand, or require a prior assumption on the exact or maximum size of the groups to be discovered.
In the following sections, we compare our results against the CoMEt algorithm \citep{comet}, which is a state-of-the-art method for discovering multiple groups of mutually exclusive mutations.
While CoMEt requires prespecifying the number and size of groups to be searched for, it is able to produce in the end a consensus of arbitrarily sized groups.

\subsection{Probabilistic Modeling of Gene Mutations}
Assume that we are given a ground set $V = \{1,\ldots,n\}$ of possible gene mutations, and a data set of $N$ patients, $\mathcal{D} \defeq (D_1,\ldots,D_N)$, where $D_i \subseteq V$ is the combination of mutations that were present in patient $i$.
The data is commonly represented in the literature using a binary alteration matrix, as shown in \figref{fig:bamats} (top), where each row represents a mutation, and each column a patient.
Our goal is to discover groups of mutations $M_1, M_2, \ldots$, $M_i \subseteq V$, with the property that mutations that belong to the same group rarely occur together in the same patient.
\figref{fig:bamats} (bottom) depicts a permuted version of the previous matrix, where the highlighted group of four mutations appears to be highly mutually exclusive.

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/example1.pdf}\\[1em]
\includegraphics[width=\textwidth]{figures/genes/example1_rep.pdf}\\[1em]
\caption{(top) An example binary alteration matrix, where each shaded entry denotes that a mutation occured in a patient. (bottom) The same matrix with rows and columns permuted to illustrate the high degree of mutual exclusivity of the highlighted group of four mutations.}
\label{fig:bamats}
\end{figure}

We propose using the patient data $\mathcal{D}$ to learn an \fldc{} model over the mutation space $V$.
Based on the definiton of the \fldc{} potential, we expect the columns of the $w$ and $v$ matrices to encode groups of repulsive and attractive mutations respectively.
For the purposes of this thesis, we propose extracting potential groups by thesholding each matrix at a specified level; these groups can then be further assessed using some of the previously proposed statistical tests.
More generally, one can perform marginal/conditional inference in the learned model to compute arbitrary probabilistic quantities that may be useful in specific biological applications.

Our approach offers several advantages over previous work.
First, it inherently uses higher-order potentials to directly capture mutation interactions of arbitrary size, without any need to specify the number or sizes of the groups in advance.
Second, in addition to mutual exclusivity, it also models mutation co-occurence, which may also be useful in cancer research \citep{yeang08,raphael14}.
Finally, in terms of computational complexity, the only potentially super-linear component in our learning procedure is the number of samples required to get an accurate gradient approximation.
As we have previously seen, proving general bounds on the mixing time is a hard endeavour, but practical experience shows that our algorithm only takes a few minutes to run on data sets of hundreds of mutations.


\section{Synthetic Data}
\todo{Show learning curve for small example}

\section{Real Cancer Data}

\subsection{Acute myeloid leukemia (AML)}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/mat_aml.pdf}\\[2em]
\caption{Test}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/aml_1.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_2.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_3.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_4.pdf}\\[2em]
\caption{Test}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/aml_5.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_6.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_7.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_8.pdf}\\[2em]
\caption{Test}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/graph_aml.pdf}\\[2em]
\caption{AML repulsive graph}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/aml_comet1.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_comet2.pdf}\\[2em]
\caption{CoMEt extra groups (probably appendix)}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/aml_2_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_1_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_3_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_5_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/aml_4_a.pdf}\\[2em]
\caption{Test}
\end{figure}

\subsection{Breast cancer (BRCA)}
\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/mat_brca.pdf}\\[2em]
\caption{Test}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/brca_1.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_6.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_8.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_2.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_5.pdf}\\[2em]
\caption{BRCA repulsive (I)}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/brca_4.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_3.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_11.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_10.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_7.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_9.pdf}\\[2em]
\caption{BRCA repulsive (II)}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/graph_brca.pdf}\\[2em]
\caption{BRCA repulsive graph}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/brca_1_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_11_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_7_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_8_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_14_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_4_a.pdf}\\[2em]
\caption{BRCA attractive}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/brca_9_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_13_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_5_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_10_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_12_a.pdf}\\[2em]
\caption{BRCA attractive (appendix I)}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=\textwidth]{figures/genes/brca_3_a.pdf}\\[2em]
%\includegraphics[width=\textwidth]{figures/genes/brca_15_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_2_a.pdf}\\[2em]
\includegraphics[width=\textwidth]{figures/genes/brca_6_a.pdf}\\[2em]
\caption{BRCA attractive (appendix II)}
\end{figure}

\section{Conclusion}
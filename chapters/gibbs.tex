\chapter{Gibbs Sampling in Prob. Submodular Models} \label{ch:gibbs}

\emph{The majority of the content of this chapter has already been published in conference proceedings \citep{gotovos15}.}

\section{Introduction}
In this chapter, we consider one of the simplest and most commonly used sampling procedures, namely the (single-site) Gibbs sampler, which is also known as the Glauber chain.
While there has been extensive work on the properties of the Gibbs sampler on low-order models, for example, Ising models \citep[Ch. 15]{levin08book}, not much is known about its behavior on higher-order models, except that, in general, we cannot hope for sub-exponential mixing times \citep{jerrum93}.
In fact, we show that even for probabilistic submodular models defined by monotone submodular functions, there are simple model families with exponential lower bounds on mixing time.

Our goal is to establish theoretical conditions that guarantee rapid mixing of the Gibbs sampler in probabilistic submodular models, and at the same time, investigate in what way the properties of sub- and supermodularity affect the resulting conditions.


\section{Problem Setup} \label{sect:setup}
In this chapter, we focus on distributions of the form
\begin{align}\label{eq:gibbs_pdef}
p(S) = \frac{\exp(\beta F(S))}{Z},
\end{align}
for all $S \subseteq V$, where $F$ is submodular or supermodular.
We currently assume that $F$ is already learned or given, and omit the parameter vector $\btheta$ from the notation.
Furthermore, we have introduced a scaling parameter $\beta \geq 0$, which is referred to as inverse temperature, and will be useful for our subsequent theoretical analysis.
Intuitively, $\beta$ controls the concentration of $p$ around the high-value sets of $F$.
When $\beta = 0$, $p$ is the uniform distribution over $2^V$; when $\beta \to \infty$, the mass of $p$ fully concentrates around the maximizers of $F$.

\paragraph{Marginal inference.}
Our goal is to perform marginal inference for the distributions described above.
Concretely, for some fixed $A \subseteq B \subseteq V$, we would like to compute the probability of sets $S$ that contain all elements of $A$, but no elements outside of $B$, that is, $p(A \subseteq S \subseteq B)$.
More generally, we are interested in computing conditional probabilities of the form $p(A \subseteq S \subseteq B \mid C \subseteq S \subseteq D)$.
This computation can be reduced to computing unconditional marginals as follows.
For any $C \subseteq V$, define the contraction of $F$ on $C$, $F_C : 2^{V \setminus C} \to \mathbb{R}$, by $F_C(S) = F(S \cup C) - F(S)$, for all $S \subseteq V \setminus C$.
Also, for any $D \subseteq V$, define the restriction of $F$ to $D$, $F^D : 2^D \to \mathbb{R}$, by $F^D(S) = F(S)$, for all $S \subseteq D$.
If $F$ is submodular, then its contractions and restrictions are also submodular, and, thus, $(F_C)^D$ is submodular.
Finally, it is easy to see that $p(S \mid C \subseteq S \subseteq D) \propto \exp(\beta (F_C)^D(S))$.
In our experiments, we consider computing marginals of the form $p(i \in S \mid C \subseteq S \subseteq D)$, for some $i \in V$, which correspond to $A = \{i\}$, and $B = V$.

\section{Hardness of Inference}
Performing exact inference in probabilistic submodular models is, in general, computationally infeasible.
Only for very few exceptions, such as determinantal point processes, is exact inference possible in polynomial time \citep{kulesza12}.
As we mentioned before, even approximating the partition function of general Ising models---a subclass of PSMs---is a hard problem; in particular, there is no FPRAS for this problem, unless RP = NP \citep{jerrum93}.
This implies that the mixing time of any Markov chain with such a stationary distribution will, in general, be exponential in $n$.

\subsection{Example: Log-submodular Grid}
To further highlight the hardness of inference in the general models we consider, we show that even for distributions defined through a seemingly benign subclass of submodular functions, mixing times can be exponential in $n$.
We say that a set function $F : 2^V \to \mathbb{R}$ is monotone, if $F(i \mid S) \geq 0$, for all $i \in V$, and all $S \subseteq V$.
Intuitively, adding elements to our set always leads to higher values.

\todo{Introduce Metropolis either here or in the background chapter.}
For the purposes of the following proposition, we will use a Metropolis chain, rather than a Gibbs chain, to simplify the exposition.
While the two chains are not identical, they share the same principle of making local moves by considering ratios of probabilities of neighboring states.
\begin{prop}
There is a family of monotone submodular functions $(F_n)_n$, such that, for the corresponding log-submodular family of distributions $(p_n)_n$ defined as in \eqref{eq:gibbs_pdef}, the Metropolis chain has mixing time
\begin{align*}
  \tm  = \Omega(2^{n/2}),
\end{align*}
for any value of $\beta$.
\end{prop}

\begin{proof}
The functions used to prove the above lemma are based on the following construction.
For any even $n \geq 2$, let $V_n = \{1,\ldots,n\}$, $R_n = \{1,\ldots,n/2\}$, and $C_n = \{n/2+1,\ldots,n\}$.
To define function $F_n : 2^{V_n} \to \mathbb{R}$, we conceptually use a $n/2 \times n/2$ square grid, whose rows are indexed by $R$ and columns by $C$.
Each cell $(i, j)$ of the grid is considered to be covered, if either row $i \in R$ or column $j \in C$ is selected.
Formally, we define $F_n$ by
\begin{align*}
  F_n(S) = \frac{4}{n^2}\big\vert \sdef{(i, j) \in R \times C}{i \in S \lor j \in S}\big\vert,
\end{align*}
for any $S \subseteq V_n$, which results in $F_n(V_n) = 1$.
\figref{fig:submod_grid} shows an example of such a grid construction.

\begin{figure}[htb]
  \centering
  \input{figures/gibbs/grid.tex}\\[1em]
  \caption{Example grid for $n = 8$ with the cells corresponding to $F_8(\{1,3,7\}) = 10/16$ shown shaded.}
  \label{fig:submod_grid}
\end{figure}

\newcommand{\hrn}{\mathcal{R}_n}
\newcommand{\hcn}{\mathcal{C}_n}
\newcommand{\hkn}{\mathcal{K}_n}
\newcommand{\htn}{\mathcal{T}_n}

Furthermore, if we define $\hrn = \{S \subseteq V \mid R \subseteq S\}$, $\hcn = \{S \mid C \subseteq S \subseteq V\}$, and $\hkn = 2^V \setminus (\hrn \cup \hcn)$, then the following properties hold.
\begin{align}
  &|\hrn| = |\hcn| = 2^{n/2} \label{eq:prop1} \\
  &\hrn \cap \hcn = \{V\} \label{eq:prop2} \\
  &\forall S \in \hrn\cup\hcn,\ f(S) = 1 \label{eq:prop3} \\
  &\forall S \in \hkn,\ f(S) \leq 1 - 4/n^2 \label{eq:prop4}.
\end{align}
Assume a Metropolis chain with transition matrix $P$, and stationary distribution $p_n(S) \propto \exp(\beta F_n(S))$.
To prove a lower bound on the mixing time of this chain, we are going to upper bound the bottleneck ratio \citep[Ch. 7]{levin08} of set $\htn = \hrn \setminus \{V\}$, defined as
\begin{align*}
  \Phi(\htn) = \frac{Q(\htn, \htn^c)}{\pi(\htn)} = \frac{Q(\htn, \hcn) + Q(\htn, \hkn)}{\pi(\htn)},
\end{align*}
where $\htn^c = 2^V \setminus \htn$ is the complement of $\htn$. We now compute or bound each of the terms $\pi(\htn)$, $Q(\htn, \hcn)$, and $Q(\htn, \hkn)$.

\paragraph{Computing $\pi(\htn)$.}
\begin{align*}
  \pi(\htn) &= |\htn| \frac{e^{\beta}}{Z} \tag*{by \eqref{eq:prop3}} \\
         &= (2^{n/2} - 1) \frac{e^{\beta}}{Z}. \tag*{by \eqref{eq:prop1}}
\end{align*}
Note that, by an analogous derivation, we get $\pi(\hcn \setminus \{V\}) = \pi(\htn)$ and, by \eqref{eq:prop2},
\begin{align*}
  &\pi(\htn) + \pi(\hcn \setminus \{V\}) < 1 \\
  \Rightarrow\ \ &\pi(\htn) < 0.5.
\end{align*}
  
\paragraph{Computing $Q(\htn, \hcn)$.}
\begin{align*}
  Q(\htn, \hcn) &= \sum_{x \in \htn} Q(x, \hcn) \\
                &= \sum_{x \in \htn} Q(x, \{V\}) \tag*{by \eqref{eq:prop2}} \\
                &= \sum_{x \in \htn} \frac{1}{2n} \frac{e^{\beta}}{Z} \tag*{by \eqref{eq:prop3}} \\
                &= n \frac{1}{2n} \frac{e^{\beta}}{Z} = \frac{e^{\beta}}{2Z}.
\end{align*}
    
\paragraph{Bounding $Q(\htn, \hkn)$.}
\begin{align*}
  Q(\htn, \hkn) &= \sum_{x \in \htn} Q(x, \hkn) \\
                &\leq \sum_{x \in \htn} \frac{1}{2n} \frac{e^{\beta - 4\beta/n^2}}{Z} \tag*{by \eqref{eq:prop4}} \\
                &= \frac{2^{n/2} - 1}{2n} \frac{e^{\beta - 4\beta/n^2}}{Z}. \tag*{by \eqref{eq:prop1}}
\end{align*}

\paragraph{Bounding $\Phi(\htn)$.}
\begin{align*}
  \Phi(\htn) &\leq \frac{1}{2^{n/2} - 1}\left(\frac{1}{2} + \frac{(2^{n/2} - 1) e^{-4\beta/n^2}}{2n}\right)\\
          &= \frac{1}{2(2^{n/2} - 1)} + \frac{e^{-4\beta/n^2}}{2n}.
\end{align*}
Using Theorem 7.3 \citep{levin08book}, it follows that
\begin{align*}
  \tm(1/4) \geq \frac{1}{4\Phi(\htn)} = \Omega(2^{n/2}).
\end{align*}

\end{proof}

\section{Polynomial-time mixing} \label{sect:poly}
Our first result provides conditions that guarantee polynomial mixing times in the size of the ground set $n$.
As we will see, the conditions depend crucially on the following quantity, which is defined for any set function $F : 2^V \to \mathbb{R}$:
\begin{align*}
  \zf \defeq \max_{A, B \subseteq V} \left|F(A) + F(B) - F(A \cup B) - F(A \cap B) \right|.
\end{align*}
Intuitively, $\zf$ quantifies a notion of distance to modularity.
For submodular and supermodular functions, $\zf$ represents the worst-case amount by which $F$ violates the submodular inequality, and $\zf = 0$ if $F$ is modular (cf. \sectref{sect:bg_submod}).

It is also important to note that, for submodular and supermodular functions, $\zf$ depends only on the monotone part of $F$; if we decompose $F$ according to \defref{def:decomp}, then it is easy to see that $\zf = \zeta_f$.
A trivial upper bound on $\zf$, therefore, is $\zf \leq f(V)$.
Another quantity that has been used in the past to quantify the deviation of a submodular function from modularity is the curvature \citep{conforti84}, defined as $\kappa_F \defeq 1 - \min_{i \in V} \left(F(i\mid V\setminus\{i\}) / F(i)\right)$.
Although of similar intuitive meaning, the multiplicative nature of its definition makes it significantly different from $\zf$, which is defined additively.

\paragraph{Examples.}
As an example of a function class with $\zf$ that do not depend on $n$, assume a ground set $V = \bigcup_{\ell = 1}^L V_{\ell}$, and consider functions of the form
\begin{align*}
F(S) = \sum_{\ell = 1}^L \phi(|S \cap V_{\ell}|),
\end{align*}
where $\phi : \mathbb{R} \to \mathbb{R}$ is a bounded concave function, for example, $\phi(x) = \min\{\phi_{\max}, x\}$.
Functions of this form are submodular, and have been used in applications such as document summarization to encourage diversity \citep{lin11}.
It is easy to see that, for such functions, $\zf \leq L\phi_{\max}$, that is, $\zf$ is independent of $n$.

For the \flid{} model (see \exampleref{ex:flid}), we have $f(S) = \sum_{j=1}^L \max_{i \in S} w_{ij}$, therefore we get $\zf \leq f(V) = \sum_{j=1}^L w_j^{\mathrm{max}}$, where $w_j^{\mathrm{max}} = \max_{i \in V} w_{ij}$.
Since the values of $\bw$ depend primarily on the number of repulsive groups, rather than the size of the ground set, we expect $\zf$ to grow much slower than $n$ in most practical applications.

\vspace{1.5em}
\noindent The following theorem establishes a bound on the mixing time of the Gibbs sampler run on models of the form \eqref{eq:gibbs_pdef}.
The bound is exponential in $\zf$, but polynomial in $n$.
\begin{theorem} \label{thm:poly}
  For any function $F : 2^V \to \mathbb{R}$, the mixing time of the Gibbs sampler is bounded by
  \begin{align*}
    \tme \leq 2n^2 \exp(2 \beta \zf) \log\left(\frac{1}{\epsilon p_{\min}}\right),
  \end{align*}
  where $p_{\min} \defeq \displaystyle\min_{S \in \ss}p(S)$.
  If $F$ is submodular or supermodular, then the bound is improved to
  \begin{align*}
    \tme \leq 2n^2 \exp(\beta \zeta_f) \log\left(\frac{1}{\epsilon p_{\min}}\right).
  \end{align*}
\end{theorem}
Note that, since the factor of two that constitutes the difference between the two statements of the theorem lies in the exponent, it can have a significant impact on the above bounds.
The dependence on $p_{\min}$ is related to the (worst-case) starting state of the chain, and can be eliminated if we have a way to guarantee a high-probability starting state.
If $F$ is submodular or supermodular, this is usually straightforward to accomplish by using one of the standard constant-factor optimization algorithms \citep{nemhauser78,fujishige05} as a preliminary step.
More generally, if $F$ is bounded by $0 \leq F(S) \leq F_{\max}$, for all $S \subseteq V$, then $\log (1/p_{\min}) = \mathcal{O}(n \beta F_{\max})$.

\paragraph{Canonical paths}
Our proof of \theoremref{thm:poly} is based on the method of \emph{canonical paths} \citep{jerrum03,sinclair92,jerrum89,diaconis91}.
The high-level idea of this method is to view the state space as a graph, and try to construct a path between each pair of states, which carries a certain amount of flow specified by the stationary distribution under consideration.
Depending on the choice of these paths and the resulting load on the edges of the graph we can derive bounds on the mixing time of the Markov chain.

More concretely, let us assume that for some set function $F$ and corresponding distribution $p$ as in \eqref{eq:gibbs_pdef}, we construct the Gibbs chain on state space $\ss = 2^V$ with transition matrix $P$.
We can view the state space as a directed graph that has vertex set $\ss$, and for any $A, B \in \ss$, contains edge $(S, S')$ if and only if $S \sim S'$, that is, if and only if $S$ and $S'$ differ by exactly one element.
Now, for any pair of states $A, B \in \ss$, we define a \emph{canonical path}
\begin{align*}
\gamma_{AB} \defeq (A = S_0, S_1, \ldots, S_{\ell} = B),
\end{align*}
such that all $(S_i, S_{i+1})$ are edges in the above graph.
We denote the length of path $\gamma_{AB}$ by $|\gamma_{AB}|$, and define $Q(S, S') \defeq p(S) P(S, S')$.
We also denote the set of all pairs of states whose canonical path goes through $(S, S')$ by
\begin{align*}
\mathcal{C}_{SS'} \defeq \sdef{(A, B) \in \ss \times \ss}{(S, S') \in \gamma_{AB}}.
\end{align*}
The following quantity, referred to as the \emph{congestion} of an edge, uses a collection of canonical paths to quantify to what amount that edge is overloaded:
\begin{align} \label{eq:cong}
  \rho(S, S') \defeq \frac{1}{Q(S, S')} \sum_{(A, B) \in \mathcal{C}_{SS'}} p(A) p(B) |\gamma_{AB}|.
\end{align}
The denominator $Q(S, S')$ quantifies the capacity of edge $(S, S')$, while the sum represents the total flow through that edge according to the choice of canonical paths.
The congestion of the whole graph is then defined as $\rho \defeq \max_{S \sim S'}\rho(S, S')$.
Low congestion implies that there are no bottlenecks in the state space, and the chain can move around fast, which results in rapid mixing.
The following theorem makes this statement more concrete.

\begin{theorem}[\hspace{1sp}\citealp{sinclair92,jerrum03}] \label{thm:cpath}
  For any collection of canonical paths with congestion $\rho$, the mixing time of the chain is bounded by
  \begin{align*}
  	\tme \leq \rho \log\left(\frac{1}{\epsilon p_{\mathrm{min}}}\right),
  \end{align*}
where $p_{\mathrm{min}} \defeq \displaystyle\min_{S \in \ss}p(S)$.
\end{theorem}

\renewcommand{\subflen}{0.48\textwidth}
\begin{figure}[tbp]
  %\captionsetup[subfigure]{oneside,margin={2em,0em}}
  \begin{subfigure}[b]{\subflen}
    \centering
    \includegraphics[width=\textwidth]{figures/gibbs/cp_easy_path_4.pdf}
    \caption{Canonical path}
    \label{fig:cong1}
  \end{subfigure}\hspace{1em}
  \begin{subfigure}[b]{\subflen}
    \centering
    \includegraphics[width=\textwidth]{figures/gibbs/cp_easy.pdf}
    \caption{Capacities}
    \label{fig:cong2}
  \end{subfigure}\\[2em]
  \begin{subfigure}[b]{\subflen}
    \centering
    \includegraphics[width=\textwidth]{figures/gibbs/cp_easy_cong.pdf}
    \caption{Low congestion}
    \label{fig:cong3}
  \end{subfigure}\hspace{1em}
  \begin{subfigure}[b]{\subflen}
    \centering
    \includegraphics[width=\textwidth]{figures/gibbs/cp_hard_cong.pdf}
    \caption{High congestion}
    \label{fig:cong4}
  \end{subfigure}\\
  \caption{(a) The state space for ground set $V = \{1, 2, 3, 4\}$, and an illustration of a canonical path from $A = \{2, 4\}$ to $B = \{1, 3\}$.
  (b) For an example distribution $p$, the width of each edge denotes the corresponding capacity $Q(S, S')$.
  (c) The color of each edge denotes the corresponding congestion $\rho(S, S')$; darker edges indicate higher congestion.
  (d) Similar to (c), but for a different distribution that has almost all of its mass concentrated on seven states.
  It can been seen that it has notably higher congestion $\rho$, and contains a significant bottleneck at $S = \{V\}$.
  }
  \label{fig:cong}
\end{figure}

\subsection{Proof of \theoremref{thm:poly}}
To apply \theoremref{thm:cpath} to our class of distributions, we need to construct a set of canonical paths in the corresponding state space $2^V$, and upper bound the resulting congestion.
First, note that, to transition from state $A \in \ss$ to state $B \in \ss$, in our case, it is enough to remove the elements of $A \setminus B$ and add the elements of $B \setminus A$.
Each removal and addition corresponds to an edge in the state space graph, and the order of these operations identify a canonical path in this graph that connects $A$ to $B$.
For our analysis, we assume a fixed order on $V$ (e.g., the natural order of the elements themselves), and perform the operations according to this order.
\figref{fig:cong1} shows an example of such a canonical path for a small state space, and \figsref{fig:cong2}--\ref{fig:cong4} illustrate the capacities $Q(S, S')$, and congestions $\rho(S, S')$ for two example distributions.

Having defined the set of canonical paths, we proceed to bounding the congestion $\rho(S, S')$ for any edge $(S, S')$.
The main difficulty in bounding $\rho(S, S')$ is due to the sum in \eqref{eq:cong} over all pairs in $\mathcal{C}_{SS'}$.
To simplify this sum, we construct for each edge $(S, S')$ an injective map $\ess : \mathcal{C}_{SS'} \to \ss$; this is a combinatorial encoding technique that has been previously used in similar proofs to ours \citep{jerrum03}.
The following lemma details this construction, where for sets $A, B$, we denote $A \oplus B \defeq (A \setminus B)\cup(B \setminus A)$.

\begin{lemma} \label{lem:inj}
Define the maps $\ess : \mathcal{C}_{SS'} \to \ss$, for each pair $(S, S') \in \ss \times \ss$ with $S \sim S'$, as follows:
\begin{align*}
  \ess(A, B) = \twopartdefo{A \oplus B \oplus S}{F(S') \geq F(S)}{A \oplus B \oplus S'}.
\end{align*}
Then, each map $\ess$ is injective.
\end{lemma}

\begin{proof}
  Assume that $F(S') \geq F(S)$, and $S' = S \cup \{r\}$, for some $r \in V$.
  Assume that we are given $C \defeq A \oplus B \oplus S$, and we want to recover $A$ and $B$.
  We will denote by $\prec$ the natural ordering of the ground set $V$.
  First, we define
  \begin{align*}
    K^- &\defeq \sdef{i \in C \oplus S}{i \prec r}\\
    K^+ &\defeq \sdef{i \in C \oplus S}{i \succ r}.
  \end{align*}
  Then, we can recover $A$ and $V$ as follows:
  \begin{align*}
    A &= S \oplus K^-\\
    B &= S' \oplus K^+.
  \end{align*}
  The case $S' = S \setminus \{r\}$, as well as the two cases for $F(S') < F(S)$ are completely analogous.
  Note that the distinction based on the value of the function has no effect on the proof here, but is technically needed for the next lemma.
  The only thing that changes between the cases is whether the element $r$ that gets added or removed in the transition $(S, S')$ belongs to $A$ or $B$, which is always straightforward to determine from the type of the transition (for additions it belongs to $B$, and for removals to $A$).
\end{proof}
\noindent We then prove the following key lemma about the maps constructed above.
\begin{lemma} \label{lem:poly_full}
  For any $S \sim S'$, and any $A, B \in \ss$, it holds that
  \begin{align*}
    p(A)p(B) \leq 2n\exp(2 \beta \zf)Q(S, S')p(\ess(A, B)).
  \end{align*}
  
  If $F$ is submodular or supermodular, then the bound is improved to
  \begin{align*}
  	p(A)p(B) \leq 2n\exp(\beta \zeta_f)Q(S, S')p(\ess(A, B)).
  \end{align*}
\end{lemma}
\begin{proof}
  We will consider the case $S' = S \cup \{r\}$, for some $r \in V$, with $F(S') \geq F(S)$.
  Again, the other three cases are completely analogous by using $\ess$ as defined in \lemmaref{lem:inj}.
  
  We first compute
  \begin{align*}
    Q(S, S') &= p(S)P(S, S')\\
             &= \frac{1}{n}\frac{p(S)p(S')}{p(S) + p(S')} \tag*{by definition of the Gibbs sampler}\\
             &= \frac{1}{nZ}\frac{\exp(\beta F(S)) \exp(\beta F(S'))}{\exp(\beta F(S)) + \exp(\beta F(S'))} \tag*{by definition of our models}\\
             &\geq \frac{1}{nZ}\frac{\exp(\beta F(S)) \exp(\beta F(S'))}{2\exp(\beta F(S'))} \tag*{by $F(S') \geq F(S)$}\\
             &= \frac{\exp(\beta F(S))}{2nZ}.
  \end{align*}
  As a result, we get
  \begin{align} \label{eq:ppq}
    \frac{p(A)p(B)}{Q(S, S')} \leq \frac{2n}{Z}\exp(\beta (F(A) + F(B) - F(S))).
  \end{align}
  Let us denote
  \begin{align*}
    \zf(A, B) \defeq F(A) + F(B) - F(A \cup B) - F(A \cap B),
  \end{align*}
  for any $A, B \subseteq V$, so that $\zf = \max_{A, B \subseteq V}|\zf(A, B)|$.
  Then, if we denote
  \begin{align*}
    C \defeq \ess(A, B) = A \oplus B \oplus S,
  \end{align*}
  we have
  \begin{align*}
    &F(A) + F(B) - F(S)\\
    &= (F(A) + F(B) - F(A \cup B) - F(A \cap B)) -\\
    &\ \ \ \ \ (F(S) + F(C) - F(A \cup B) - F(A \cap B)) + F(C)\\
    &= (F(A) + F(B) - F(A \cup B) - F(A \cap B)) -\\
    &\ \ \ \ \ (F(S) + F(C) - F(S \cup C) - F(S \cap C)) + F(C)\\
    &= \zf(A, B) - \zf(S, C) + F(C)\\
    &\leq 2\zf + F(C).
  \end{align*}
  If $F$ is submodular, then $\zf(A, B)$ and $\zf(S, C)$ are both non-negative, therefore $\zf(A, B) - \zf(S, C) + F(C) \leq \zf + F(C) = \zeta_f + F(C)$.
  Similarly, if $F$ is supermodular, then $\zf(A, B)$ and $\zf(S, C)$ are both non-positive, therefore $\zf(A, B) - \zf(S, C) + F(C) \leq \zf + F(C) = \zeta_f + F(C)$.
  Substituting these bounds in \eqref{eq:ppq} gives us the result of the lemma.
\end{proof}
Since $\ess$ is injective, it follows that $\sum_{(A, B) \in \mathcal{C}_{SS'}} p(\ess(A, B)) \leq 1$.
Furthermore, it is clear that each canonical path $\gamma_{AB}$ has length $|\gamma_{AB}| \leq n$, since we need to add and/or remove at most $n$ elements to get from state $A$ to state $B$.
Combining these two facts with the above lemma, we get
\begin{align*}
  \rho(S, S') \leq 2n^2 \exp(2 \beta \zf),
\end{align*}
for any set function $F$, and
\begin{align*}
  \rho(S, S') \leq 2n^2 \exp(2 \beta \zeta_f),
\end{align*}
if $F$ is sub- or supermodular.


\section{Fast mixing}
We now proceed to show that, under some stronger conditions, we are able to establish even faster---$\mathcal{O}(n \log n)$---mixing.
For any function $F$, we denote
\begin{align*}
\D_F(i\mid S) \defeq F(S \cup \{i\}) - F(S \setminus \{i\}),
\end{align*}
and define the following quantity,
\begin{align*}
  \gf &\defeq \max_{\substack{S \subseteq V\\r \in V}} \sum_{\substack{i \in V}} \tanh\left(\frac{\beta}{2} \Big|\D_F(i\mid S) - \D_F(i\mid S \cup \{r\})\Big| \right),
\end{align*}
which quantifies the (maximum) total influence of an element $r \in V$ on the values of $F$.
For example, if the inclusion of $r$ makes no difference with respect to other elements of the ground set, we will have $\gf = 0$.
The following theorem establishes conditions for fast mixing of the Gibbs sampler when run on models of the form \eqref{eq:gibbs_pdef}.

\begin{theorem} \label{thm:fast}
  For any set function $F : 2^V \to \mathbb{R}$, if $\gf < 1$, then the mixing time of the Gibbs sampler is bounded by
  \begin{align*}
  	\tme \leq \frac{1}{1 - \gf}n(\log n + \log \frac{1}{\epsilon}).
  \end{align*}
  If $F$ is additionally submodular or supermodular, and is decomposed according to \defref{def:decomp}, then
  \begin{align*}
  	\tme \leq \frac{1}{1 - \gsf}n(\log n + \log \frac{1}{\epsilon}).
  \end{align*}
\end{theorem}
Note that, in the second part of the theorem, $\gsf$ depends only on the monotone part of $F$. 

\paragraph{Coupling.}
Our proof of \theoremref{thm:fast} is based on the \emph{coupling} technique \citep{aldous83}; more specifically, we use the \emph{path coupling} method \citep{bubley97,levin08,jerrum03}.
Given a Markov chain $(Z_t)$ on state space $\ss$ with transition matrix $P$, a coupling for $(Z_t)$ is a new Markov chain $(X_t, Y_t)$ on state space $\ss \times \ss$, such that both $(X_t)$ and $(Y_t)$ are by themselves Markov chains with transition matrix $P$.
The idea is to construct the coupling in such a way that, even when the starting points $X_0$ and $Y_0$ are different, the chains $(X_t)$ and $(Y_t)$ tend to coalesce.
Then, it can be shown that the coupling time $t_{\mathrm{couple}} \defeq \min\sdef{t \geq 0}{X_t = Y_t}$ is closely related to the mixing time of the original chain $(Z_t)$ \citep{levin08}.

The main difficulty in applying the coupling approach lies in the construction of the coupling itself, for which one needs to consider any possible pair of states $(X_t, Y_t)$.
The path coupling technique makes this construction easier by utilizing the same state-space graph that we used to define canonical paths in \sectref{sect:poly}.
The core idea is to first define a coupling only over adjacent states, and then extend it for any pair of states by using a metric on the graph.
More concretely, let us denote by $d : \ss \times \ss \to \mathbb{R}$ the \emph{path metric} on state space $\ss$; that is, for any $x, y \in \ss$, $d(x, y)$ is the minimum length of any path from $x$ to $y$ in the state space graph.
The following theorem establishes fast mixing using this metric, as well as the diameter of the state space, $\mathrm{diam}(\ss) \defeq \max_{x,y \in \ss}d(x, y)$.
\begin{theorem}[\hspace{1sp}\citealp{bubley97,levin08}] \label{thm:pc}
For any Markov chain $(Z_t)$, let $(X_t, Y_t)$ be a coupling, such that, for some $a \geq 0$, and any $x, y \in \ss$ with $x \sim y$, it holds that
\begin{align*}
  \E[d(X_{t+1}, Y_{t+1}) \mid X_t = x, Y_t = y] \leq e^{-\alpha}d(x, y).
\end{align*}
Then, the mixing time of the original chain $(Z_t)$ is bounded by
\begin{align*}
  \tme \leq \frac{1}{\alpha}\left(\log(\mathrm{diam}(\ss)) + \log\frac{1}{\epsilon} \right).
\end{align*}
\end{theorem}

\subsection{Proof of \theoremref{thm:fast}}
In our case, the path metric $d$ is the Hamming distance between the binary vectors representing the states (equivalently, the number of elements by which two sets differ).
We need to construct a suitable coupling $(X_t, Y_t)$ for any pair of states $x \sim y$.
Consider the two corresponding sets $S, R \subseteq V$ that differ by exactly one element, and assume that $R = S \cup \{r\}$, for some $r \in V$. (The case $S = R \cup \{s\}$ for some $s \in V$ is completely analogous.)
Remember that the Gibbs sampler first chooses an element $i \in V$ uniformly at random, and then adds or removes it according to the conditional probabilities.
Our goal is to make the same updates happen to both $S$ and $R$ as frequently as possible.
As a first step, we couple the candidate element for update $i \in V$ to always be the same in both chains.
Then, we have to distinguish between the following cases.

If $i = r$, then the conditionals for both chains are identical, and we can couple both chains to add $r$ with probability
\begin{align*}
p_{\mathrm{add}} \defeq \frac{p(S \cup \{r\})}{p(S) + p(S \cup \{r\})},
\end{align*}
which will result in new sets $S' = R' = S \cup \{r\}$, or remove $r$ with probability $1 - p_{\mathrm{add}}$, which will result in new sets $S' = R' = S$.
Either way, we will have $d(S', R') = 0$.
  
If $i \neq r$, we cannot always couple the updates of the chains, because the conditional probabilities of the updates are different.
In fact, we are forced to have different updates---one chain adding $i$, the other chain removing $i$---with probability equal to the difference of the corresponding conditionals, which we denote here by $p_{\mathrm{dif}}(v)$, defined as follows,
\begin{align*}
  p_{\mathrm{dif}}(i) \defeq \left|\frac{p(S \cup \{i\})}{p(S \cup \{i\}) + p(S \setminus \{i\})} - \frac{p(R \cup \{i\})}{p(R \cup \{i\}) + p(R \setminus \{i\})}\right|.
\end{align*}
In the case of different updates, we have $d(S', R') = 2$, otherwise the chains make the same update and still differ only by element $r$, that is, $d(S', R') = 1$.

Putting together the three possible cases for the value of $d(S', R')$ described above, we get the following expected distance after one step,
\begin{align*}
  \E[d(S', R')] = 1 -\frac{1}{n} + \frac{1}{n}\sum_{i \neq r}p_{\mathrm{dif}}(i).
\end{align*}
We then prove the following lemma to bound the sum of $p_{\mathrm{dif}}$.
\begin{lemma}
For any $S, R \subseteq V$ with $R = S \cup \{r\}$,
\begin{align*}
  \sum_{i \neq r}p_{\mathrm{dif}}(i) \leq \gf.
\end{align*}
\end{lemma}

\begin{proof}
  For any $i \neq r$, we have
  \begin{align*}
    p_{\mathrm{dif}}(i) &= \Bigg|\frac{\exp(\beta F(S \cup \{i\}))}{\exp(\beta F(S \cup \{i\})) + \exp(\beta F(S \setminus \{i\}))} -\\
    &\ \ \ \ \ \ \ \frac{\exp(\beta F(R \cup \{i\}))}{\exp(\beta F(R \cup \{i\})) + \exp(\beta F(R \setminus \{i\}))}\Bigg|\\[0.8em]
    &= \left|\frac{\exp(\beta \D_F(i\mid S))}{1 + \exp(\beta \D_F(i\mid S))} - \frac{\exp(\beta \D_F(i\mid R))}{1 + \exp(\beta \D_F(i\mid R))}\right|\\[0.8em]
    &= \left|\frac{\exp(\beta \D_F(i\mid S)) - \exp(\beta \D_F(i\mid R))}{(1 + \exp(\beta \D_F(i\mid S)))(1 + \exp(\beta \D_F(i\mid R)))}\right|\\[0.8em]
    &\leq \left|\frac{\exp(\beta \D_F(i\mid S)) - \exp(\beta \D_F(i\mid R))}{\exp(\beta \D_F(i\mid S)) + \exp(\beta \D_F(i\mid R))}\right|\\[0.8em]
    &= \left|\frac{\exp(\beta (\D_F(i\mid S) - \D_F(i\mid R))) - 1}{\exp(\beta (\D_F(i\mid S) - \D_F(i\mid R))) + 1}\right|\\[0.8em]
    &= \tanh \left(\frac{\beta}{2}\big|(\D_F(i\mid S) - \D_F(i\mid R)) \big|\right).
  \end{align*}
  The lemma follows by the definition of $\gf$, and the fact that $R = S \cup \{r\}$.
\end{proof}
\noindent Applying this lemma, we get
\begin{align*}
  \E[d(S', R')] = 1 -\frac{1}{n} + \frac{1}{n}\sum_{i \neq r}p_{\mathrm{dif}}(i) \leq 1 - \frac{1}{n}(1 - \gf) \leq \exp\left(-\frac{1-\gf}{n}\right),
\end{align*}
and the result of \theoremref{thm:fast} follows from applying \theoremref{thm:pc} with $\alpha = \gf/n$, and noting that $\mathrm{diam}(\ss) = n$.

The specialization of \theoremref{thm:fast} to sub- or supermodular functions is based on the following lemma.
\begin{lemma}
  If $F$ is submodular or supermodular, and decomposed according to \eqref{eq:decomp}, then
  \begin{align*}
    \gf = \gsf.
  \end{align*}
\end{lemma}

\begin{proof}
  For any $S, R \subseteq V$ with $R = S \cup \{r\}$, and any $i \in V$, we have
  \begin{align*}
    \D_F(i\mid S) - \D_F(i\mid R) &= F(S \cup \{i\}) - F(S \setminus \{i\}) - F(R \cup \{i\}) + F(R \setminus \{i\})\\
    &= f(S \cup \{i\}) - f(S \setminus \{i\}) - f(R \cup \{i\}) + f(R \setminus \{i\})\\
    &= \D_f(i\mid S) - \D_f(i\mid R).
  \end{align*}
\end{proof}

\subsection{Additively Decomposable Functions}
Some commonly used models, such as the Ising model and \flid{}, can be written as a sum of simpler supermodular (resp. submodular) functions $F_{j}$,
\begin{align} \label{eq:fdec}
F(S) = \sum_{j \in [L]} F_{j}(S).
\end{align}
We prove the following corollary that provides an easy to check condition for fast mixing of the Gibbs sampler when $F$ can be additively decomposed as above.
\begin{cor} \label{cor:fast}
  For any submodular function $F$ that can be written in the form of \eqref{eq:fdec}, with $f$ being its monotone (also additively decomposable) part according to \defref{def:decomp}, if we define
  \begin{align*}
  	\theta_f \defeq \max_{i \in V} \sum_{j\in [L]} \sqrt{f_{j}(\{i\})} \hspace{1em}\textrm{and}\hspace{1em} \lambda_f \defeq \max_{j\in [L]} \sum_{i \in V} \sqrt{f_{j}(\{i\})},
  \end{align*}
  then it holds that
  \begin{align*}
  	\gsf \leq \frac{\beta}{2} \theta_f \lambda_f.
  \end{align*}
\end{cor}

\begin{proof}
  For any $S, R \subseteq V$ with $R = S \cup \{r\}$, we have
  \begin{align*}
  	&\sum_{i \neq r} \tanh \left(\frac{\beta}{2}\big|(\D_f(i\mid S) - \D_f(i\mid R)) \big|\right)\\
    &\leq \sum_{i \neq r} \frac{\beta}{2}\big|(\D_f(i\mid S) - \D_f(i\mid R)) \big| \tag*{by $\tanh(x) \leq x$, for all $x \geq 0$}\\
    &\leq \sum_{i \neq r} \frac{\beta}{2}(\D_f(i\mid S) - \D_f(i\mid R)) \tag*{by submodularity of $f$}\\
    &= \frac{\beta}{2}\sum_{i \neq r}(f(S \cup \{i\}) - f(S \setminus \{i\}) - f(S \cup \{r\} \cup \{i\}) + f(S \cup \{r\} \setminus \{i\}))\\
    &= \frac{\beta}{2}\sum_{i \neq r}\sum_{j \in [L]}(f_{j}(S \cup \{i\}) - f_{j}(S \setminus \{i\}) - f_{j}(S \cup \{r\} \cup \{i\}) + f_{j}(S \cup \{r\} \setminus \{i\}))\\
    &\leq \frac{\beta}{2}\sum_{i \neq r}\sum_{j \in [L]}\min\big\{f_{j}(S \cup \{i\}) - f_{j}(S \setminus \{i\}), f_{j}(S \cup \{r\} \setminus \{i\}) - f_{j}(S \setminus \{i\})\big\} \tag*{by monotonicity of $f_{j}$}\\
    &\leq \frac{\beta}{2}\sum_{i \neq r}\sum_{j \in [L]}\min\big\{f_{j}(i), f_{j}(r)\big\} \tag*{by submodularity of $f_{j}$}\\
    &\leq \frac{\beta}{2}\sum_{i \neq r}\sum_{j \in [L]}\sqrt{f_{j}(i) f_{j}(r)}\\
    &= \frac{\beta}{2}\sum_{j \in [L]}\sqrt{f_{j}(r)}\sum_{i \neq r}\sqrt{ f_{j}(i)}.
  \end{align*}
  The result follows by maximizing both sides over $S$ and $r$.
\end{proof}

\paragraph{Example.}Applying the above corollary to the \flid{} model, we get
\begin{align*}
\theta_f = \max_{i \in V} \sum_{j \in [L]} \sqrt{w_{ij}},
\end{align*}
and
\begin{align*}
\lambda_f = \max_{j \in [L]} \sum_{i \in V} \sqrt{w_{ij}},
\end{align*}
and we obtain fast mixing if $\theta_f \lambda_f \leq 2/\beta$.
As a special case, if we consider the class of set cover functions ($w_{ij} \in \{0, 1\}$), such that each $i \in V$ covers at most $\delta$ sets, and each set indicated by $j \in [L]$ is covered by at most $\delta$ elements, then $\theta_f, \lambda_f \leq \delta$, and we obtain fast mixing if $\delta^2 \leq 2/\beta$.
Note, that the corollary can be trivially applied to any submodular function by taking $L=1$, but may, in general, result in a loose bound if used that way.

\section{Experiments}
In the following two experiments, we compare the Gibbs sampler against the variational approach proposed by \cite{djolonga14} for performing inference in probabilistic submodular models.
In particular, the authors propose two variational approximations, denoted in the following by ``upper'' and ``lower'', which are obtained from factorized distributions associated with modular upper and lower bounds respectively.

\paragraph{Estimating the log-partition function.}
We start with approximating the normalizers $\log(Z)$ for a family of (log-submodular) \flid{} models on ground set sizes ranging from $n = 10$ to $n = 100$.
These \flid{} models are learned from synthetic data that we describe in \sectref{sect:syn_single}.
In short, each model represents a single approximately mutually exclusive group of three genes together with five frequently and independently occurring genes, as well as a number of random noise genes.

We obtain estimates for $\log(Z)$ via a Gibbs-sampler based reverse importance sampling procedure \todo{ref background}, using $200$, $1000$, and $5000$ samples.
For each model we repeat the sampling procedure $100$ times to get standard error estimates.
Since estimating the exact value of $\log(Z)$ is infeasible for $n > 20$, we obtain an accurate estimate by computing the averaged importance sampling and reverse importance sampling estimates when run with $2\cdot 10^6$ samples.
\figref{fig:gibbs_zest} shows the estimation errors with respect to this approximately true value; errorbars depict two standard errors.
As is natural, more Gibbs samples result in more accurate estimates, and we can also observe that reverse importance sampling tends to produce overestimates of the log-partition function.
We also see that the two variational approaches, which guarantee upper and lower bounds respectively, are considerably less accurate.

\setlength\figureheight{0.6\textwidth}
\setlength\figurewidth{0.8\textwidth}
\renewcommand{\subflen}{\textwidth}
\begin{figure}[tb]
  \centering
  \input{figures/gibbs/zest.tex}
  \caption{The error in estimating the log-partition function when using Gibbs-based reverse importance sampling compared to the variational approximations by \cite{djolonga14}.}
  \label{fig:gibbs_zest}
\end{figure}

\paragraph{Estimating marginals.}
We now repeat the experiments performed by \cite{djolonga14} to estimate marginals, and use the same three models that they used.

The first is a (log-submodular) \flid{} model, in which the manually added modular term penalizes the number of selected elements, that is, $p(S) \propto \exp(f(S)-2|S|)$, where $f$ is a submodular facility location function.
The model is constructed from randomly subsampling real data from a problem of sensor placement in a water distribution network \citep{krause08}.
In the experiments, we iteratively condition on random observations for each variable in the ground set.

The second is a (log-supermodular) pairwise Markov random field, constructed by first randomly sampling points from a two-dimensional two-cluster Gaussian mixture model, and then introducing a pairwise potential for each pair of points with exponentially-decreasing weight in the distance of the pair.
In the experiments, we iteratively condition on pairs of observations, one from each cluster.

The third is a (log-supermodular) higher-order Markov random field, which is constructed by first generating a random Watts-Strogatz graph, and then creating one higher-order potential per node, which contains that node and all of its neighbors in the graph.
The strength of the potentials is controlled by a parameter $\mu$, which is closely related to the curvature of the functions that define them.
In the experiments, we vary this parameter from $0$ (modular model) to $1$ (``strongly'' supermodular model).

For all three models, we constrain the size of the ground set to $n = 20$, so that we are able to compute, and compare against, the exact marginals.
Furthermore, we run multiple repetitions for each model to account for the randomness of the model instance, and the random initialization of the Gibbs sampler.
The marginals we compute are of the form $p(i \in S \mid C \subseteq S \subseteq D)$, for all $i \in V$.
As before, we run the Gibbs sampler for $200$, $1000$, and $5000$ iterations on each problem instance.

\figref{fig:gibbs_exp} compares the average absolute error of the approximate marginals with respect to the exact ones.
The averaging is performed over $i \in V$, and over the different repetitions of each experiment; errorbars depict two standard errors.
We notice a similar trend on all three models.
For the regimes that correspond to less ``peaked'' posterior distributions (small number of conditioned variables, small $\mu$), even a few thousand Gibbs iterations outperform both variational approximations.
On the other hand, the variational methods gain an advantage when the posterior is concentrated around only a few states, which happens after having conditioned on almost all variables in the first two models, or for $\mu$ close to $1$ in the third model.

\setlength\figureheight{0.45\textwidth}
\setlength\figurewidth{0.7\textwidth}
\renewcommand{\subflen}{\textwidth}
\begin{figure}[tb]
  \captionsetup[subfigure]{oneside,margin={2em,0em}}
  \begin{subfigure}[b]{\subflen}
    \centering
    \input{figures/gibbs/floc.tex}
    \caption{Facility location}
    \label{fig:gibbs_exp1}
  \end{subfigure}\\[0.5em]
  \begin{subfigure}[b]{\subflen}
    \centering
    \input{figures/gibbs/gmm.tex}
    \caption{Pairwise MRF}
    \label{fig:gibbs_exp2}
  \end{subfigure}\\[0.5em]
  \begin{subfigure}[b]{\subflen}
    \centering
    \input{figures/gibbs/ic.tex}
    \caption{Higher-order MRF}
    \label{fig:gibbs_exp3}
  \end{subfigure}\\[-1em]
  \caption{Absolute error of the marginals computed by the Gibbs sampler compared to the variational approximations by \cite{djolonga14}.}
  \label{fig:gibbs_exp}
\end{figure}


\section{Conclusion}
In this chapter, we presented sufficient conditions that guarantee upper bounds on the mixing time of the Gibbs sampler in general probabilistic submodular models.
Furthermore, we demonstrated that, in practice, the Gibbs sampler compares favorably to previously proposed variational approximations, at least in regimes of high uncertainty.

\paragraph{Further related work.}
In contemporary work to ours, Rebeschini and Karbasi \citep{rebeschini15} analyzed the mixing times of log-submodular models.
Using a method based on matrix norms, which was previously introduced by \cite{dyer09}, and is closely related to path coupling, they arrived at a similar, though not directly comparable, condition to the one we presented in \theoremref{thm:fast}.

\cite{li16} extended our polynomial-time mixing result of \theoremref{thm:poly} to the problem of sampling from a distribution under specific constraints.
In particular, they used a similar to ours canonical path argument involving an analogous quantity to our $\zf$ to prove mixing bounds under uniform and partition matroid constraints.

The canonical path method for bounding mixing times has been previously used in a number of theoretical results, such as approximating the partition function of ferromagnetic Ising models \citep{jerrum93}, approximating matrix permanents \citep{jerrum89,jerrum04perm}, and counting matchings in graphs \citep{jerrum03}.

Coupling-based methods have been most prominently used for counting $k$-colorings in low-degree graphs \citep{jerrum95,bubley98,jerrum03}.
Other applications of coupling include counting independent sets in graphs \citep{dyer00}, and approximating the partition function of various subclasses of Ising models at high temperatures \citep{levin08book}.
\chapter{Gibbs Sampling in PSMs} \label{ch:gibbs}
Modeling notions such as coverage, representativeness, or diversity is an important challenge in many machine learning problems.
These notions are well captured by submodular set functions.
Analogously, supermodular functions capture notions of smoothness, regularity, or cooperation. 
As a result, submodularity and supermodularity, akin to concavity and convexity, have found numerous applications in machine learning.
The majority of previous work has focused on optimizing such functions, including the development and analysis of algorithms for minimization \cite{fujishige05} and maximization \cite{nemhauser78,feige07}, as well as the investigation of practical applications, such as sensor placement \cite{krause06}, active learning \cite{golovin11}, influence maximization \cite{kempe03}, and document summarization \cite{lin11}.

Beyond optimization, though, it is of interest to consider probabilistic models defined via submodular functions, that is, distributions over finite sets (or, equivalently, binary random vectors) defined as $p(S) \propto \exp(\beta F(S)),$ where $F : 2^V \to \mathbb{R}$ is a submodular or supermodular function (equivalently, either $F$ or $-F$ is submodular), and $\beta \geq 0$ is a scaling parameter.
Finding most likely sets in such models captures classical submodular optimization.
However, going beyond point estimates, that is, performing general probabilistic (e.g., marginal) inference in them, allows us to quantify uncertainty given some observations, as well as learn such models from data.
Only few special cases belonging to this class of models have been extensively studied in the past; most notably, Ising models \cite{koller09}, which are log-supermodular in the usual case of attractive (ferromagnetic) potentials, or log-submodular under repulsive (anti-ferromagnetic) potentials, and determinantal point processes \cite{kulesza12}, which are log-submodular.

Recently, \citet{djolonga14} considered a more general treatment of such models, and proposed a variational approach for performing approximate probabilistic inference for them.
It is natural to ask to what degree the usual alternative to variational methods, namely Monte Carlo sampling, is applicable to these models, and how it performs in comparison.
To this end, in this paper we consider a simple Markov chain Monte Carlo (MCMC) algorithm on log-submodular and log-supermodular models, and provide a first analysis of its performance.
We present two theoretical conditions that respectively guarantee polynomial-time and fast ($\mathcal{O}(n \log n)$) mixing in such models, and experimentally compare against the variational approximations on three examples.

\section{Problem Setup} \label{sect:setup}
We start by considering set functions $F : 2^V \to \mathbb{R}$, where $V$ is a finite ground set of size $|V| = n$.
Without loss of generality, if not otherwise stated, we will hereafter assume that $V = [n] \defeq \{1, 2, \ldots,n\}$.
The marginal gain obtained by adding element $v \in V$ to set $S \subseteq V$ is defined as $F(v|S) \defeq F(S \cup \{v\}) - F(S)$.
Intuitively, submodularity expresses a notion of diminishing returns; that is, adding an element to a larger set provides less benefit than adding it to a smaller one.
More formally, $F$ is submodular if, for any $S \subseteq T \subseteq V$, and any $v \in V \setminus T$, it holds that $F(v|T) \leq F(v|S)$.
Supermodularity is defined analogously by reversing the sign of this inequality.
In particular, if a function $F$ is submodular, then the function $-F$ is supermodular.
If a function $m$ is both submodular and supermodular, then it is called modular, and may be written in the form $m(S) = c + \sum_{v \in S} m_v$, where $c \in \mathbb{R}$, and $m_v \in \mathbb{R}$, for all $v \in V$.

Our main focus in this paper are distributions over the powerset of $V$ of the form
\begin{align}\label{eq:pdef}
p(S) = \frac{\exp(\beta F(S))}{Z},
\end{align}
for all $S \subseteq V$, where $F$ is submodular or supermodular.
The scaling parameter $\beta$ is referred to as inverse temperature, and distributions of the above form are called log-submodular or log-supermodular respectively.
The constant denominator $Z \defeq Z(\beta) \defeq \sum_{S \subseteq V} \exp(\beta F(S))$ serves the purpose of normalizing the distribution and is called the partition function of $p$.
An alternative and equivalent way of defining distributions of the above form is via binary random vectors $X \in \{0, 1\}^n$.
If we define $V(X) \defeq \sdef{v \in V}{X_v = 1}$, it is easy to see that the distribution $p_X(X) \propto \exp(\beta F(V(X)))$ over binary vectors is isomorphic to the distribution over sets of \eqref{eq:pdef}.
With a slight abuse of notation, we will use $F(X)$ to denote $F(V(X))$, and use $p$ to refer to both distributions.

\paragraph{Example models}
The (ferromagnetic) Ising model is an example of a log-supermodular model.
In its simplest form, it is defined through an undirected graph $(V, E)$, and a set of pairwise potentials $\sigma_{v,w}(S) \defeq 4(\mathds{1}_{\{v \in S\}}-0.5)(\mathds{1}_{\{w \in S\}}-0.5)$.
Its distribution has the form $p(S) \propto \exp(\beta\sum_{\{v,w\} \in E} \sigma_{v,w}(S))$, and is log-supermodular, because $F(S) = \sum_{\{v,w\} \in E} \sigma_{v,w}(S)$ is supermodular. (Each $\sigma_{v,w}$ is supermodular, and supermodular functions are closed under addition.)

Determinantal point processes (DPPs) are examples of log-submodular models.
A DPP is defined via a positive semidefinite matrix $K \in \mathbb{R}^{n \times n}$, and has a distribution of the form $p(S) \propto \det(K_S)$, where $K_S$ denotes the square submatrix indexed by $S$.
Since $F(S) = \ln \det(K_S)$ is a submodular function, $p$ is log-submodular.
Another example of log-submodular models are those defined through facility location functions, which have the form $F(S) = \sum_{\ell \in [L]} \max_{v \in S}w_{v,\ell}$, where $w_{v,\ell} \geq 0$, and are submodular.
If $w_{v,\ell} \in \{0, 1\}$, then $F$ represents a set cover function.

Note that, both the facility location model and the Ising model use decomposable functions, that is, functions that can be written as a sum of simpler submodular (resp. supermodular) functions $F_{\ell}$:
\begin{align} \label{eq:fdec}
F(S) = \sum_{\ell \in [L]} F_{\ell}(S).
\end{align}

\paragraph{Marginal inference}
Our goal is to perform marginal inference for the distributions described above.
Concretely, for some fixed $A \subseteq B \subseteq V$, we would like to compute the probability of sets $S$ that contain all elements of $A$, but no elements outside of $B$, that is, $p(A \subseteq S \subseteq B)$.
More generally, we are interested in computing conditional probabilities of the form $p(A \subseteq S \subseteq B \mid C \subseteq S \subseteq D)$.
This computation can be reduced to computing unconditional marginals as follows.
For any $C \subseteq V$, define the contraction of $F$ on $C$, $F_C : 2^{V \setminus C} \to \mathbb{R}$, by $F_C(S) = F(S \cup C) - F(S)$, for all $S \subseteq V \setminus C$.
Also, for any $D \subseteq V$, define the restriction of $F$ to $D$, $F^D : 2^D \to \mathbb{R}$, by $F^D(S) = F(S)$, for all $S \subseteq D$.
If $F$ is submodular, then its contractions and restrictions are also submodular, and, thus, $(F_C)^D$ is submodular.
Finally, it is easy to see that $p(S \mid C \subseteq S \subseteq D) \propto \exp(\beta (F_C)^D(S))$.
In our experiments, we consider computing marginals of the form $p(v \in S \mid C \subseteq S \subseteq D)$, for some $v \in V$, which correspond to $A = \{v\}$, and $B = V$.

\section{Sampling and Mixing Times}
Performing exact inference in models defined by \eqref{eq:pdef} boils down to computing the partition function $Z$.
Unfortunately, this is generally a \#P-hard problem, which was shown to be the case even for Ising models by Jerrum and Sinclair \cite{jerrum93}.
However, they also proposed a sampling-based FPRAS for a class of ferromagnetic models, which gives us hope that it may be possible to efficiently perform approximate inference in more general models under suitable conditions.

MCMC sampling \cite{levin08} approaches are based on performing randomly selected local moves in a state space $\ss$ to approximately compute quantities of interest.
The visited states $(X_0, X_1,\ldots)$ form a Markov chain, which under mild conditions converges to a stationary distribution $\pi$.
Crucially, the probabilities of transitioning from one state to another are carefully chosen to ensure that the stationary distribution is identical to the distribution of interest.
In our case, the state space is the powerset of $V$ (equivalently, the space of all binary vectors of length $n$), and to approximate the marginal probabilities of $p$ we construct a chain over subsets of $V$ that has stationary distribution $p$.

\paragraph{The Gibbs sampler}
In this paper, we focus on one of the simplest and most commonly used chains, namely the Gibbs sampler, also known as the Glauber chain.
We denote by $P$ the transition matrix of the chain; each element $P(x, y)$ corresponds to the conditional probability of transitioning from state $x$ to state $y$, that is, $P(x, y) \defeq \P[X_{t+1} = y \mid X_{t} = x]$, for any $x, y \in \ss$, and any $t \geq 0$.
We also define an adjacency relation $x \sim y$ on the elements of the state space, which denotes that $x$ and $y$ differ by exactly one element.
It follows that each $x \in \ss$ has exactly $n$ neighbors.

\begin{algorithm}[tb]
	\caption{Gibbs sampler}
	\label{alg:gibbs}
	\small{
		\begin{algorithmic}[1]
			\REQUIRE Ground set $V$, distribution $p(S) \propto \exp(\beta F(S))$
			\LET{$X_0$}{random subset of $V$}
			\FOR{$t = 0$ \TO $N_{\mathrm{iter}}$}
			\LET{$v$}{$\mathrm{Unif}(V)$}
			\LET{$\Delta_F(v | X_t)$}{$F(X_t \cup \{v\}) - F(X_t \setminus \{v\})$}
			\LET{$p_{\mathrm{add}}$}{$\exp(\beta\Delta_F(v | X_t))/(1 + \exp(\beta\Delta_F(v | X_t)))$}
			\LET{$z$}{$\mathrm{Unif}([0,1])$}
			\LINEIFELSE{$z \leq p_{\mathrm{add}}$}{$X_{t+1} \gets X_t \cup \{v\}$}{$X_{t+1} \gets X_t \setminus \{v\}$}
			\ENDFOR
		\end{algorithmic}
	}
\end{algorithm}

The Gibbs sampler is defined by an iterative two-step procedure, as shown in \algoref{alg:gibbs}.
First, it selects an element $v \in V$ uniformly at random; then, it adds or removes $v$ to the current state $X_t$ according to the conditional probability of the resulting state.
Importantly, the conditional probabilities that need to be computed do not depend on the partition function $Z$, thus the chain can be simulated efficiently, even though $Z$ is unknown and hard to compute.
Moreover, it is easy to see that $\Delta_F(v | X_t) = \mathds{1}_{\{v\not\in X_t\}}F(v|X_t) + \mathds{1}_{\{v\in X_t\}}F(v|X_t\setminus\{v\})$; thus, the sampler only requires a black box for the marginal gains of $F$, which are often faster to compute than the values of $F$ itself.
Finally, it is easy to show that the stationary distribution of the chain constructed this way is $p$.

\paragraph{Mixing times}
Approximating quantities of interest using MCMC methods is based on using time averages to estimate expectations over the desired distribution.
In particular, we estimate the expected value of function $f : \ss \to \mathbb{R}$ by $\E_p[f(X)] \approx (1/T)\sum_{r=1}^{T} f(X_{s + r})$.
For example, to estimate the marginal $p(v \in S)$, for some $v \in V$, we would define $f(x) = \mathds{1}_{\{x_v = 1\}}$, for all $x \in \ss$.
The choice of burn-in time $s$ and number of samples $T$ in the above expression presents a tradeoff between computational efficiency and approximation accuracy.
It turns out that the effect of both $s$ and $T$ is largely dependent on a fundamental quantity of the chain called \emph{mixing time} \cite{levin08}.

The mixing time of a chain quantifies the number of iterations $t$ required for the distribution of $X_t$ to be close to the stationary distribution $\pi$.
More formally, it is defined as $\tme \defeq \min \sdef{t}{d(t) \leq \epsilon}$, where $d(t)$ denotes the worst-case (over the starting state $X_0$ of the chain) total variation distance between the distribution of $X_t$ and $\pi$.
Establishing upper bounds on the mixing time of our Gibbs sampler is, therefore, sufficient to guarantee efficient approximate marginal inference (e.g., see Theorem 12.19 of \citet{levin08}).

\section{Theoretical Results}
In the previous section we mentioned that exact computation of the partition function for the class of models we consider here is, in general, infeasible.
Only for very few exceptions, such as DPPs, is exact inference possible in polynomial time \cite{kulesza12}.
Even worse, it has been shown that the partition function of general Ising models is hard to approximate; in particular, there is no FPRAS for these models, unless RP = NP \cite{jerrum93}.
This implies that the mixing time of any Markov chain with such a stationary distribution will, in general, be exponential in $n$.
It is, therefore, our aim to derive sufficient conditions that guarantee sub-exponential mixing times for the general class of models.

In some of our results we will use the fact that any submodular function $F$ can be written as
\begin{align} \label{eq:decomp}
  F = c + m + f,
\end{align}
where $c \in \mathbb{R}$ is a constant that has no effect on distributions defined by \eqref{eq:pdef}; $m$ is a normalized ($m(\varnothing) = 0$) modular function; and $f$ is a normalized ($f(\varnothing) = 0$) monotone submodular function, that is, it additionally satisfies the monotonicity property $f(v|S) \geq 0$, for all $v \in V$, and all $S \subseteq V$.
A similar decomposition is possible for any supermodular function as well.

%To further highlight the hardness of inference in the general models we consider, we show that even for distributions defined through a seemingly benign subclass of submodular functions, mixing times can be exponential in $n$.
%We say that a set function $F : 2^V \to \mathbb{R}$ is monotone, if $F(v|S) \geq 0$, for all $v \in V$, and all $S \subseteq V$.
%Intuitively, adding elements to our set always leads to higher values.
%\begin{lemma}
%There is a family of monotone submodular functions $F_n$, such that, for the corresponding log-submodular family of distributions $p_n$ defined as in \eqref{eq:pdef}, the Metropolis chain has mixing time
%\begin{align*}
%  \tm  = \Omega(2^{n/2}),
%\end{align*}
%\end{lemma}
%for any value of $\beta$.
%
%The functions used to prove the above lemma are based on the following construction.
%For any even $n \geq 2$, let $V_n = \{1,\ldots,n\}$, $R_n = \{1,\ldots,n/2\}$, and $C_n = \{n/2+1,\ldots,n\}$.
%To define function $F_n : 2^{V_n} \to \mathbb{R}$, we conceptually use a $n/2 \times n/2$ square grid, whose rows are indexed by $R$ and columns by $C$.
%Each cell $(i, j)$ of the grid is considered to be covered if either row $i \in R$ or column $j \in C$ is selected.
%Formally, we define $F_n$ by
%\begin{align*}
%  F_n(S) = \frac{4}{n^2}\big\vert \sdef{(i, j) \in R \times C}{i \in S \lor j \in S}\big\vert,
%\end{align*}
%for any $S \subseteq V_n$, which results in $F_n(V_n) = 1$.
%
%Assume a Metropolis chain with transition matrix $P$, and stationary distribution $p_n(S) \propto \exp(\beta F_n(S))$.
%
%\newcommand{\subflen}{0.49\textwidth}
%\newcommand{\scspacey}{0em}
%\newcommand{\scspacex}{0em}
%\begin{figure}[tb]
%  \captionsetup[subfigure]{oneside,margin={2em,0em}}
%  \begin{subfigure}[b]{\subflen}
%	\centering
%	\begin{tikzpicture}[
%	%baseline,
%	scale=1
%	]
%	\fill[blue!40!white] (-2,1) rectangle (2,2);
%	\fill[blue!40!white] (-2,-1) rectangle (2,0);
%	\fill[blue!40!white] (0,-2) rectangle (1,2);
%	\draw[step=1cm,gray!80!black,line width=0.15em,line join=miter] (-2.01,-2.01) grid (2.01,2.01);
%	
%	\node at (-2.5,1.5) {\normalsize$1$};
%	\node at (-2.5,0.5) {\normalsize$2$};
%	\node at (-2.5,-0.5) {\normalsize$3$};
%	\node at (-2.5,-1.5) {\normalsize$4$};
%	\node at (-1.5,2.5) {\normalsize$5$};
%	\node at (-0.5,2.5) {\normalsize$6$};
%	\node at (0.5,2.5) {\normalsize$7$};
%	\node at (1.5,2.5) {\normalsize$8$};
%	\end{tikzpicture}
%	%\caption{}
%	\label{fig:grid}
%  \end{subfigure}
%  \begin{subfigure}[b]{\subflen}
%  	\centering
%    \trimbox{3em 6em 3em 6em}{
%  	\begin{tikzpicture}[
%  	%baseline,
%  	scale=1,
%    VN/.style={fill=white,draw=gray!80!black,circle,line width=0.15em,minimum size=1.5em},
%    LL/.style={draw=gray!80!black,line width=0.1em}
%    ]
%    %\clip (-6,-4) rectangle (-6, -4);
%    \draw [LL,fill=gray!30!white] (0,0) .. controls (4,4) and (4,-4) .. (0,0);
%    \draw [LL,fill=gray!30!white] (0,0) .. controls (-4,4) and (-4,-4) .. (0,0);
%    \node [VN] (V) at (0,0) {$V$};
%    \node (R) at (-1.8, 0) {$\mathcal{R}$};
%    \node (C) at (1.8, 0) {$\mathcal{C}$};
%  	\end{tikzpicture}}
%  	%\caption{}
%  	\label{fig:ss}
%  \end{subfigure}
%  \caption{\emph{Left.} Example grid for $n = 8$ with the cells corresponding to $F_8(\{1,3,7\})$ shown shaded. \emph{Right.} Illustration of the state space and the bottleneck at $V$.}
%\end{figure}
%
\subsection{Polynomial-time mixing} \label{sect:poly}
Our guarantee for mixing times that are polynomial in $n$ depends crucially on the following quantity, which is defined for any set function $F : 2^V \to \mathbb{R}$:
\begin{align*}
  \zf \defeq \max_{A, B \subseteq V} \left|F(A) + F(B) - F(A \cup B) - F(A \cap B) \right|.
\end{align*}
Intuitively, $\zf$ quantifies a notion of distance to modularity.
To see this, note that a function $F$ is modular if and only if $F(A) + F(B) = F(A \cup B) + F(A \cap B)$, for all $A, B \subseteq V$.
For modular functions, therefore, we have $\zf = 0$.
Furthermore, a function $F$ is submodular if and only if $F(A) + F(B) \geq F(A \cup B) + F(A \cap B)$, for all $A, B \subseteq V$.
Similarly, $F$ is supermodular if the above holds with the sign reversed.
It follows that for submodular and supermodular functions, $\zf$ represents the worst-case amount by which $F$ violates the modular equality.
It is also important to note that, for submodular and supermodular functions, $\zf$ depends only on the monotone part of $F$; if we decompose $F$ according to \eqref{eq:decomp}, then it is easy to see that $\zf = \zeta_f$.
A trivial upper bound on $\zf$, therefore, is $\zf \leq f(V)$.
Another quantity that has been used in the past to quantify the deviation of a submodular function from modularity is the curvature \cite{conforti84}, defined as $\kappa_F \defeq 1 - \min_{v \in V} \left(F(v|V\setminus\{v\}) / F(v)\right)$.
Although of similar intuitive meaning, the multiplicative nature of its definition makes it significantly different from $\zf$, which is defined additively.

As an example of a function class with $\zf$ that do not depend on $n$, assume a ground set $V = \bigcup_{\ell = 1}^L V_{\ell}$, and consider functions $F(S) = \sum_{\ell = 1}^L \phi(|S \cap V_{\ell}|)$, where $\phi : \mathbb{R} \to \mathbb{R}$ is a bounded concave function, for example, $\phi(x) = \min\{\phi_{\max}, x\}$.
Functions of this form are submodular, and have been used in applications such as document summarization to encourage diversity \cite{lin11}.
It is easy to see that, for such functions, $\zf \leq L\phi_{\max}$, that is, $\zf$ is independent of $n$.

The following theorem establishes a bound on the mixing time of the Gibbs sampler run on models of the form \eqref{eq:pdef}.
The bound is exponential in $\zf$, but polynomial in $n$.
\begin{theorem} \label{thm:poly}
  For any function $F : 2^V \to \mathbb{R}$, the mixing time of the Gibbs sampler is bounded by
  \begin{align*}
    \tme \leq 2n^2 \exp(2 \beta \zf) \log\left(\frac{1}{\epsilon p_{\min}}\right),
  \end{align*}
  where $p_{\min} \defeq \displaystyle\min_{S \in \ss}p(S)$.
  If $F$ is submodular or supermodular, then the bound is improved to
  \begin{align*}
    \tme \leq 2n^2 \exp(\beta \zeta_f) \log\left(\frac{1}{\epsilon p_{\min}}\right).
  \end{align*}
\end{theorem}
Note that, since the factor of two that constitutes the difference between the two statements of the theorem lies in the exponent, it can have a significant impact on the above bounds.
The dependence on $p_{\min}$ is related to the (worst-case) starting state of the chain, and can be eliminated if we have a way to guarantee a high-probability starting state.
If $F$ is submodular or supermodular, this is usually straightforward to accomplish by using one of the standard constant-factor optimization algorithms \cite{nemhauser78,fujishige05} as a preprocessing step.
More generally, if $F$ is bounded by $0 \leq F(S) \leq F_{\max}$, for all $S \subseteq V$, then $\log (1/p_{\min}) = \mathcal{O}(n \beta F_{\max})$.

\paragraph{Canonical paths}
Our proof of \theoremref{thm:poly} is based on the method of \emph{canonical paths} \cite{jerrum03,sinclair92,jerrum89,diaconis91}.
The high-level idea of this method is to view the state space as a graph, and try to construct a path between each pair of states that carries a certain amount of flow specified by the stationary distribution under consideration.
Depending on the choice of these paths and the resulting load on the edges of the graph, we can derive bounds on the mixing time of the Markov chain.

More concretely, let us assume that for some set function $F$ and corresponding distribution $p$ as in \eqref{eq:pdef}, we construct the Gibbs chain on state space $\ss = 2^V$ with transition matrix $P$.
We can view the state space as a directed graph that has vertex set $\ss$, and for any $A, B \in \ss$, contains edge $(S, S')$ if and only if $S \sim S'$, that is, if and only if $S$ and $S'$ differ by exactly one element.
Now, assume that, for any pair of states $A, B \in \ss$, we define what is called a canonical path $\gamma_{AB} \defeq (A = S_0, S_1, \ldots, S_{\ell} = B)$, such that all $(S_i, S_{i+1})$ are edges in the above graph.
We denote the length of path $\gamma_{AB}$ by $|\gamma_{AB}|$, and define $Q(S, S') \defeq p(S) P(S, S')$.
We also denote the set of all pairs of states whose canonical path goes through $(S, S')$ by $\mathcal{C}_{SS'} \defeq \sdef{(A, B) \in \ss \times \ss}{(S, S') \in \gamma_{AB}}$.
The following quantity, referred to as the \emph{congestion} of an edge, uses a collection of canonical paths to quantify to what amount that edge is overloaded:
\begin{align} \label{eq:cong}
  \rho(S, S') \defeq \frac{1}{Q(S, S')} \sum_{(A, B) \in \mathcal{C}_{SS'}} p(A) p(B) |\gamma_{AB}|.
\end{align}
The denominator $Q(S, S')$ quantifies the capacity of edge $(S, S')$, while the sum represents the total flow through that edge according to the choice of canonical paths.
The congestion of the whole graph is then defined as $\rho \defeq \max_{S \sim S'}\rho(S, S')$.
Low congestion implies that there are no bottlenecks in the state space, and the chain can move around fast, which also suggests rapid mixing.
The following theorem makes this concrete.

\begin{theorem}[\hspace{1sp}\cite{sinclair92,jerrum03}] \label{thm:cpath}
  For any collection of canonical paths with congestion $\rho$, the mixing time of the chain is bounded by
  \begin{align*}
  	\tme \leq \rho \log\left(\frac{1}{\epsilon p_{\mathrm{min}}}\right).
  \end{align*}
%  where $p_{\min} \defeq \displaystyle\min_{S \in \ss}p(S)$.
\end{theorem}

\paragraph{Proof outline of \theoremref{thm:poly}}
To apply \theoremref{thm:cpath} to our class of distributions, we need to construct a set of canonical paths in the corresponding state space $2^V$, and upper bound the resulting congestion.
First, note that, to transition from state $A \in \ss$ to state $B \in \ss$, in our case, it is enough to remove the elements of $A \setminus B$ and add the elements of $B \setminus A$.
Each removal and addition corresponds to an edge in the state space graph, and the order of these operations identify a canonical path in this graph that connects $A$ to $B$.
For our analysis, we assume a fixed order on $V$ (e.g., the natural order of the elements themselves), and perform the operations according to this order.

Having defined the set of canonical paths, we proceed to bounding the congestion $\rho(S, S')$ for any edge $(S, S')$.
The main difficulty in bounding $\rho(S, S')$ is due to the sum in \eqref{eq:cong} over all pairs in $\mathcal{C}_{SS'}$.
To simplify this sum we construct for each edge $(S, S')$ an injective map $\ess : \mathcal{C}_{SS'} \to \ss$; this is a combinatorial encoding technique that has been previously used in similar proofs to ours \cite{jerrum03}.
We then prove the following key lemma about these maps.

\begin{lemma} \label{lem:poly}
  For any $S \sim S'$, and any $A, B \in \ss$, it holds that
  \begin{align*}
  	p(A)p(B) \leq 2n\exp(2 \beta \zf)Q(S, S')p(\ess(A, B)).
  \end{align*}
\end{lemma}

Since $\ess$ is injective, it follows that $\sum_{(A, B) \in \mathcal{C}_{SS'}} p(\ess(A, B)) \leq 1$.
Furthermore, it is clear that each canonical path $\gamma_{AB}$ has length $|\gamma_{AB}| \leq n$, since we need to add and/or remove at most $n$ elements to get from state $A$ to state $B$.
Combining these two facts with the above lemma, we get
\begin{align*}
  \rho(S, S') \leq 2n^2 \exp(2 \beta \zf).
\end{align*}
If $F$ is submodular or supermodular, we show that the dependence on $\zf$ in \lemmaref{lem:poly}  is improved to $\exp(\beta \zf)$.
More details can be found in the longer version of the paper.

\subsection{Fast mixing}
We now proceed to show that, under some stronger conditions, we are able to establish even faster---$\mathcal{O}(n \log n)$---mixing.
For any function $F$, we denote $\D_F(v|S) \defeq F(S \cup \{v\}) - F(S \setminus \{v\})$, and define the following quantity,
\begin{align*}
  \gf &\defeq \max_{\substack{S \subseteq V\\r \in V}} \sum_{\substack{v \in V}} \tanh\left(\frac{\beta}{2} \big|\D_F(v|S) - \D_F(v|S \cup \{r\})\big| \right),
\end{align*}

which quantifies the (maximum) total influence of an element $r \in V$ on the values of $F$.
For example, if the inclusion of $r$ makes no difference with respect to other elements of the ground set, we will have $\gf = 0$.
The following theorem establishes conditions for fast mixing of the Gibbs sampler when run on models of the form \eqref{eq:pdef}.

\begin{theorem} \label{thm:fast}
  For any set function $F : 2^V \to \mathbb{R}$, if $\gf < 1$, then the mixing time of the Gibbs sampler is bounded by
  \begin{align*}
  	\tme \leq \frac{1}{1 - \gf}n(\log n + \log \frac{1}{\epsilon}).
  \end{align*}
  If $F$ is additionally submodular or supermodular, and is decomposed according to \eqref{eq:decomp}, then
  \begin{align*}
  	\tme \leq \frac{1}{1 - \gsf}n(\log n + \log \frac{1}{\epsilon}).
  \end{align*}
\end{theorem}
Note that, in the second part of the theorem, $\gsf$ depends only on the monotone part of $F$. 
We have seen in \sectref{sect:setup} that some commonly used models are based on decomposable functions that can be written in the form \eqref{eq:fdec}.
We prove the following corollary that provides an easy to check condition for fast mixing of the Gibbs sampler when $F$ is a decomposable submodular function.

\begin{cor} \label{cor:fast}
  For any submodular function $F$ that can be written in the form of \eqref{eq:fdec}, with $f$ being its monotone (also decomposable) part according to \eqref{eq:decomp}, if we define
  \begin{align*}
  	\theta_f \defeq \max_{v \in V} \sum_{\ell \in [L]} \sqrt{f_{\ell}(v)} \hspace{1em}\textrm{and}\hspace{1em} \lambda_f \defeq \max_{\ell \in [L]} \sum_{v \in V} \sqrt{f_{\ell}(v)},
  \end{align*}
  then it holds that
  \begin{align*}
  	\gsf \leq \frac{\beta}{2} \theta_f \lambda_f.
  \end{align*}
\end{cor}

For example, applying this to the facility location model defined in \sectref{sect:setup}, we get $\theta_f = \max_{v} \sum_{\ell = 1}^L \sqrt{w_{v, \ell}}$, and $\lambda_f = \max_{\ell} \sum_{v \in V} \sqrt{w_{v, \ell}}$, and obtain fast mixing if $\theta_f \lambda_f \leq 2/\beta$.
As a special case, if we consider the class of set cover functions ($w_{v, \ell} \in \{0, 1\}$), such that each $v \in V$ covers at most $\delta$ sets, and each set $\ell \in [L]$ is covered by at most $\delta$ elements, then $\theta_f, \lambda_f \leq \delta$, and we obtain fast mixing if $\delta^2 \leq 2/\beta$.
Note, that the corollary can be trivially applied to any submodular function by taking $L=1$, but may, in general, result in a loose bound if used that way.

\paragraph{Coupling}
Our proof of \theoremref{thm:fast} is based on the \emph{coupling} technique \cite{aldous83}; more specifically, we use the \emph{path coupling} method \cite{bubley97,levin08,jerrum03}.
Given a Markov chain $(X_t)$ on state space $\ss$ with transition matrix $P$, a coupling for $(Z_t)$ is a new Markov chain $(X_t, Y_t)$ on state space $\ss \times \ss$, such that both $(X_t)$ and $(Y_t)$ are by themselves Markov chains with transition matrix $P$.
The idea is to construct the coupling in such a way that, even when the starting points $X_0$ and $Y_0$ are different, the chains $(X_t)$ and $(Y_t)$ tend to coalesce.
Then, it can be shown that the coupling time $t_{\mathrm{couple}} \defeq \min\sdef{t \geq 0}{X_t = Y_t}$ is closely related to the mixing time of the original chain $(Z_t)$. \cite{levin08}

The main difficulty in applying the coupling approach lies in the construction of the coupling itself, for which one needs to consider any possible pair of states $(Y_t, Z_t)$.
The path coupling technique makes this construction easier by utilizing the same state-space graph that we used to define canonical paths in \sectref{sect:poly}.
The core idea is to first define a coupling only over adjacent states, and then extend it for any pair of states by using a metric on the graph.
More concretely, let us denote by $d : \ss \times \ss \to \mathbb{R}$ the \emph{path metric} on state space $\ss$; that is, for any $x, y \in \ss$, $d(x, y)$ is the minimum length of any path from $x$ to $y$ in the state space graph.
The following theorem establishes fast mixing using this metric, as well as the diameter of the state space, $\mathrm{diam}(\ss) \defeq \max_{x,y \in \ss}d(x, y)$.
\begin{theorem}[\hspace{1sp}\cite{bubley97,levin08}] \label{thm:pc}
For any Markov chain $(Z_t)$, if $(X_t, Y_t)$ is a coupling, such that, for some $a \geq 0$, and any $x, y \in \ss$ with $x \sim y$, it holds that
\begin{align*}
  \E[d(X_{t+1}, Y_{t+1}) \mid X_t = x, Y_t = y] \leq e^{-\alpha}d(x, y),
\end{align*}
then the mixing time of the original chain is bounded by
\begin{align*}
  \tme \leq \frac{1}{\alpha}\left(\log(\mathrm{diam}(\ss)) + \log\frac{1}{\epsilon} \right).
\end{align*}
\end{theorem}

\paragraph{Proof outline of \theoremref{thm:fast}}
In our case, the path metric $d$ is the Hamming distance between the binary vectors representing the states (equivalently, the number of elements by which two sets differ).
We need to construct a suitable coupling $(X_t, Y_t)$ for any pair of states $x \sim y$.
Consider the two corresponding sets $S, R \subseteq V$ that differ by exactly one element, and assume that $R = S \cup \{r\}$, for some $r \in V$. (The case $S = R \cup \{s\}$ for some $s \in V$ is completely analogous.)
Remember that the Gibbs sampler first chooses an element $v \in V$ uniformly at random, and then adds or removes it according to the conditional probabilities.
Our goal is to make the same updates happen to both $S$ and $R$ as frequently as possible.
As a first step, we couple the candidate element for update $v \in V$ to always be the same in both chains.
Then, we have to distinguish between the following cases.

If $v = r$, then the conditionals for both chains are identical, therefore we can couple both chains to add $r$ with probability $p_{\mathrm{add}} \defeq p(S \cup \{r\}) / (p(S) + p(S \cup \{r\}))$, which will result in new sets $S' = R' = S \cup \{r\}$, or remove $r$ with probability $1 - p_{\mathrm{add}}$, which will result in new sets $S' = R' = S$.
Either way, we will have $d(S', R') = 0$.
  
If $v \neq r$, we cannot always couple the updates of the chains, because the conditional probabilities of the updates are different.
In fact, we are forced to have different updates (one chain adding $v$, the other chain removing $v$) with probability equal to the difference of the corresponding conditionals, which we denote here by $p_{\mathrm{dif}}(v)$.
%  \begin{align*}
%  	p_{\mathrm{dif}}(v) \defeq \left|\frac{p(S \cup \{v\})}{p(S \cup \{v\}) + p(S \setminus \{v\})} - \frac{p(R \cup \{v\})}{p(R \cup \{v\}) + p(R \setminus \{v\})}\right|.
%  \end{align*}
If this is the case, we will have $d(S', R') = 2$, otherwise the chains will make the same update and will still differ only by element $r$, that is, $d(S', R') = 1$.
  
Putting together all the above, we get the following expected distance after one step:
\begin{align*}
  \E[d(S', R')] = 1 -\frac{1}{n} + \frac{1}{n}\sum_{v \neq r}p_{\mathrm{dif}}(v) \leq 1 - \frac{1}{n}(1 - \gf) \leq \exp\left(-\frac{1-\gf}{n}\right).
\end{align*}
Our result follows from applying \theoremref{thm:pc} with $\alpha = \gf/n$, noting that $\mathrm{diam}(\ss) = n$.

\section{Experiments}
We compare the Gibbs sampler against the variational approach proposed by Djolonga and Krause \cite{djolonga14} for performing inference in models of the form \eqref{eq:pdef}, and use the same three models as in their experiments.
We briefly review here the experimental setup and refer to their paper for more details.

The first is a (log-submodular) facility location model with an added modular term that penalizes the number of selected elements, that is, $p(S) \propto \exp(F(S)-2|S|)$, where $F$ is a submodular facility location function.
The model is constructed from randomly subsampling real data from a problem of sensor placement in a water distribution network \cite{krause08}.
In the experiments, we iteratively condition on random observations for each variable in the ground set.
The second is a (log-supermodular) pairwise Markov random field (MRF; a generalized Ising model with varying weights), constructed by first randomly sampling points from a 2-D two-cluster Gaussian mixture model, and then introducing a pairwise potential for each pair of points with exponentially-decreasing weight in the distance of the pair.
In the experiments, we iteratively condition on pairs of observations, one from each cluster.
The third is a (log-supermodular) higher-order MRF, which is constructed by first generating a random Watts-Strogatz graph, and then creating one higher-order potential per node, which contains that node and all of its neighbors in the graph.
The strength of the potentials is controlled by a parameter $\mu$, which is closely related to the curvature of the functions that define them.
In the experiments, we vary this parameter from $0$ (modular model) to $1$ (``strongly'' supermodular model).

For all three models, we constrain the size of the ground set to $n = 20$, so that we are able to compute, and compare against, the exact marginals.
Furthermore, we run multiple repetitions for each model to account for the randomness of the model instance, and the random initialization of the Gibbs sampler.
The marginals we compute are of the form $p(v \in S \mid C \subseteq S \subseteq D)$, for all $v \in V$.
We run the Gibbs sampler for $100$, $500$, and $2000$ iterations on each problem instance.
In compliance with recommended MCMC practice \cite{gelman11}, we discard the first half of the obtained samples as burn-in, and only use the second half for estimating the marginals.

\figref{fig:exp} compares the average absolute error of the approximate marginals with respect to the exact ones.
The averaging is performed over $v \in V$, and over the different repetitions of each experiment; errorbars depict two standard errors.
The two variational approximations are obtained from factorized distributions associated with modular lower and upper bounds respectively \cite{djolonga14}.
We notice a similar trend on all three models.
For the regimes that correspond to less ``peaked'' posterior distributions (small number of conditioned variables, small $\mu$), even $100$ Gibbs iterations outperform both variational approximations.
The latter gain an advantage when the posterior is concentrated around only a few states, which happens after having conditioned on almost all variables in the first two models, or for $\mu$ close to $1$ in the third model.

\setlength\figureheight{0.285\textwidth}
\setlength\figurewidth{0.4\textwidth}
\newcommand{\subflen}{0.33\textwidth}
\newcommand{\scspacey}{-1.55em}
\newcommand{\scspacex}{0em}
\begin{figure}[tb]
  \captionsetup[subfigure]{oneside,margin={2em,0em}}
  \begin{subfigure}[b]{\subflen}
    \centering
    \input{figures/gibbs/floc.tex}
    \vspace{\scspacey}
    \caption{\hspace{\scspacex}Facility location}
    \label{fig:exp1}
  \end{subfigure}
  \begin{subfigure}[b]{\subflen}
    \input{figures/gibbs/gmm.tex}
    \vspace{\scspacey}
    \caption{\hspace{\scspacex}Pairwise MRF}
    \label{fig:exp2}
  \end{subfigure}
  \begin{subfigure}[b]{\subflen}
    \input{figures/gibbs/ic.tex}
    \vspace{\scspacey}
    \caption{\hspace{\scspacex}Higher-order MRF}
    \label{fig:exp3}
  \end{subfigure}
  \caption{Absolute error of the marginals computed by the Gibbs sampler compared to variational inference \cite{djolonga14}.
  	A modest $500$ Gibbs iterations outperform the variational method for the most part.}
  \label{fig:exp}
\end{figure}

\section{Further Related Work}
In contemporary work to ours, Rebeschini and Karbasi \cite{rebeschini15} analyzed the mixing times of log-submodular models.
Using a method based on matrix norms, which was previously introduced by Dyer et al. \cite{dyer09}, and is closely related to path coupling, they arrive at a similar---though not directly comparable---condition to the one we presented in \theoremref{thm:fast}.

Iyer and Bilmes \cite{iyer15} recently considered a different class of probabilistic models, called submodular point processes, which are also defined through submodular functions, and have the form $p(S) \propto F(S)$.
They showed that inference in SPPs is, in general, also a hard problem, and provided approximations and closed-form solutions for some subclasses.

The canonical path method for bounding mixing times has been previously used in applications, such as approximating the partition function of ferromagnetic Ising models \cite{jerrum93}, approximating matrix permanents \cite{jerrum89,jerrum04perm}, and counting matchings in graphs \cite{jerrum03}.
The most prominent application of coupling-based methods is counting $k$-colorings in low-degree graphs \cite{jerrum95,bubley98,jerrum03}.
Other applications include counting independent sets in graphs \cite{dyer00}, and approximating the partition function of various subclasses of Ising models at high temperatures \cite{levin08}.

\section{Conclusion}
We considered the problem of performing marginal inference using MCMC sampling techniques in probabilistic models defined through submodular functions.
In particular, we presented for the first time sufficient conditions to obtain upper bounds on the mixing time of the Gibbs sampler in general log-submodular and log-supermodular models.
Furthermore, we demonstrated that, in practice, the Gibbs sampler compares favorably to previously proposed variational approximations, at least in regimes of high uncertainty.
We believe that this is an important step towards a unified framework for further analysis and practical application of this rich class of probabilistic submodular models.